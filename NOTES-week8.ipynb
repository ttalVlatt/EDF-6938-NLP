{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "530144c4",
      "metadata": {
        "id": "530144c4"
      },
      "source": [
        "<h1><center>  lab 8 : ML Overview: Supervised Learning algorithms </center>\n",
        "    \n",
        "<img src=\"https://files.realpython.com/media/NLP-for-Beginners-Pythons-Natural-Language-Toolkit-NLTK_Watermarked.16a787c1e9c6.jpg\" width=\"400\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3c7066e",
      "metadata": {
        "id": "c3c7066e"
      },
      "source": [
        "```Created by Jinnie Shin (jinnie.shin@coe.ufl.edu)```\\\n",
        "```Date: ```\n",
        "\n",
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQmNf86oJnfhpkPA9LnrFnAbfwF2VywPYpB_w&usqp=CAU\" align=\"left\" width=\"70\" height=\"70\" align=\"left\">\n",
        "\n",
        " ### Required Packages or Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "02eac3c7",
      "metadata": {
        "id": "02eac3c7"
      },
      "outputs": [],
      "source": [
        "#!pip install { } ! in case you run into the `package not avaialble` error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98aabe31",
      "metadata": {
        "id": "98aabe31"
      },
      "source": [
        "\n",
        "## **REVIEW**: Dataset\n",
        "\n",
        "> We will use the coh-metrix indices introduced in Week 6, `features.xlsx`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c324cee8",
      "metadata": {
        "id": "c324cee8"
      },
      "outputs": [],
      "source": [
        "data= pd.read_excel('features.xlsx')\n",
        "\n",
        "############################### MINI TASKS ####################################\n",
        "# Q1. The total number of rows?\n",
        "\n",
        "# Q2. How many coh-metrix features?\n",
        "# (excluding, `TextID`, `domain1_score`, `domain2_score`, `essay_id`, and `essay_set`)\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "X = data.drop(columns=['TextID','domain1_score', 'domain2_score', 'essay_id', 'essay_set'])\n",
        "y = data.domain1_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee132932",
      "metadata": {
        "id": "ee132932"
      },
      "source": [
        "## 1. Regression and Classification Problems\n",
        "\n",
        "> Our task is to predict the `domain1_score` column using the given coh-metrix features.\n",
        "> We will implement and use the two algorithms, linear and logistic regression, as our main prediction/classification models. Before we construct the algorithms next week, we will take a look at how the model weights are learned using **the gradient descent algorithms**.\n",
        "\n",
        "### 1.1 Gradient Descent\n",
        "<img src=\"https://miro.medium.com/proxy/1*fBxEzbzP1KkqR7PTexJZdw.png\" width=\"250\">\n",
        "\n",
        "> The objective of the learning algorithm is to determine the best possible values for the parameters (`w` and `b`), such that the overall loss (squared error loss) of the model is minimized as much as possible. \\\n",
        "> Let's solve this regression problem: `y = 4.0+(3.0洧논0)+(1.0洧논1)+(3.0洧논2)+(0.5洧논3)+(1.5洧논4)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2e5ad077",
      "metadata": {
        "id": "2e5ad077"
      },
      "outputs": [],
      "source": [
        "## Define number of samples\n",
        "num_samples = 20\n",
        "\n",
        "x0 = 3.0 + np.random.standard_normal(num_samples)\n",
        "x1 = 1.0 + np.random.standard_normal(num_samples)\n",
        "x2 = -8.0 + np.random.standard_normal(num_samples)\n",
        "x3 = -2.0 + np.random.standard_normal(num_samples)\n",
        "x4 = 0.5 + np.random.standard_normal(num_samples)\n",
        "y = 4.0 + 3.0 * x0 + 1.0 * x1 + 3.0 * x2 + 0.5 * x3 + 1.5 * x4 + np.random.standard_normal(num_samples)\n",
        "\n",
        "X = np.column_stack((x0, x1, x2, x3, x4))\n",
        "Y = y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "938556da",
      "metadata": {
        "id": "938556da"
      },
      "source": [
        "#### 1.1.1 Batch Gradient Descent (BGD)\n",
        "> Partial derivates of `b` and `w` in linear regression with the squared loss is:\n",
        "<img src=\"https://eli.thegreenplace.net/images/math/aef02f077919896478d0456619f934dcc5809142.png\" width=\"250\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3453809b",
      "metadata": {
        "id": "3453809b"
      },
      "outputs": [],
      "source": [
        "def BGD(X, Y, b, w, alpha=0.005): # alpha is a learning rate, we will set it as 0.005 for now\n",
        "\n",
        "    num_feat = X.shape[1]\n",
        "\n",
        "    num_sample = X.shape[0] # This indicates the total number of data points (rows)\n",
        "\n",
        "    b_grad = 0 #Intercept\n",
        "\n",
        "    w_grad = np.zeros(num_feat) # weight vector\n",
        "\n",
        "    for i in range(num_sample): # BGD first calculates the `b_grad` or `w_grad`\n",
        "                                # from the total sample N\n",
        "        y = Y[i] # one sample, y\n",
        "        x = X[i] # one sample, x\n",
        "        b_grad += -(2./float(num_sample)) * (y - (b + w.dot(x)))\n",
        "\n",
        "        for j in range(num_feat):\n",
        "            x_ij = x[j]\n",
        "            w_grad[j] += -(2./float(num_sample)) * x_ij * (y - (b + w.dot(x)))\n",
        "\n",
        "    b_new = b - alpha * b_grad\n",
        "    w_new = np.array([w[i] - alpha * w_grad[i] for i in range(num_feat)])\n",
        "    return b_new, w_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "dccd0db9",
      "metadata": {
        "id": "dccd0db9"
      },
      "outputs": [],
      "source": [
        "def BGD_train(X, Y, alpha=0.005):\n",
        "    b = 0\n",
        "    w = np.zeros(X.shape[1])\n",
        "    print('===== Start Training ====')\n",
        "    for i in range(10000):\n",
        "        b_new, w_new = BGD(X, Y, b, w, alpha=alpha)\n",
        "        b = b_new\n",
        "        w = w_new\n",
        "        if i % 1000 == 0:\n",
        "            print('{}: b = {}, w = {}'.format(i, np.round(b_new, 2), np.round(w_new, 2)))\n",
        "\n",
        "    print('final: b = {}, w = {}'.format(np.round(b, 2), np.round(w, 2)))\n",
        "    return b, w"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93d82dc5",
      "metadata": {
        "id": "93d82dc5"
      },
      "source": [
        "> *Let's explore!*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "29b5e930",
      "metadata": {
        "id": "29b5e930",
        "outputId": "62826c7d-cf99-4c30-d5a9-1770ea721e06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Start Training ====\n",
            "0: b = -0.09, w = [-0.31 -0.11  0.8   0.2  -0.02]\n",
            "1000: b = 0.22, w = [2.73 0.9  2.38 0.51 1.17]\n",
            "2000: b = 0.53, w = [2.75 0.89 2.43 0.49 1.18]\n",
            "3000: b = 0.82, w = [2.74 0.89 2.46 0.5  1.17]\n",
            "4000: b = 1.08, w = [2.73 0.89 2.48 0.5  1.17]\n",
            "5000: b = 1.31, w = [2.72 0.89 2.5  0.51 1.16]\n",
            "6000: b = 1.52, w = [2.72 0.89 2.53 0.51 1.16]\n",
            "7000: b = 1.72, w = [2.71 0.88 2.54 0.52 1.16]\n",
            "8000: b = 1.89, w = [2.71 0.88 2.56 0.52 1.15]\n",
            "9000: b = 2.05, w = [2.7  0.88 2.58 0.52 1.15]\n",
            "final: b = 2.2, w = [2.7  0.88 2.59 0.53 1.15]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.199463971499102,\n",
              " array([2.69550846, 0.87949332, 2.59251754, 0.52785271, 1.1455164 ]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "BGD_train(X, Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80fb9bc0",
      "metadata": {
        "id": "80fb9bc0"
      },
      "source": [
        "#### 1.1.1 Stochastic Gradient Descent (SGD)\n",
        "> Shuffles the data and randomly sample one data point to update the gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "55c260d1",
      "metadata": {
        "id": "55c260d1"
      },
      "outputs": [],
      "source": [
        "def SGD(x, y, b, w, num_feat, num_sample, alpha=0.005):\n",
        "\n",
        "    b_grad = -(2./float(num_sample)) * (y - (b + w.dot(x)))\n",
        "    w_grad = np.zeros(num_feat)\n",
        "\n",
        "    for i in range(num_feat):\n",
        "        w_grad[i] += -(2./float(num_sample)) * x[i] * (y - (b + w.dot(x)))\n",
        "\n",
        "    b_new = b - alpha * b_grad\n",
        "    w_new = np.array([w[i] - alpha * w_grad[i] for i in range(num_feat)])\n",
        "    return b_new, w_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "8fae0cd2",
      "metadata": {
        "id": "8fae0cd2"
      },
      "outputs": [],
      "source": [
        "def SGD_train(X, Y, alpha =0.005):\n",
        "\n",
        "    import random\n",
        "\n",
        "    b = 0\n",
        "    w = np.zeros(X.shape[1])\n",
        "\n",
        "    num_sample = X.shape[0]\n",
        "    num_feat = X.shape[1]\n",
        "\n",
        "    for i in range(5000):\n",
        "        indices = list(range(num_sample))\n",
        "        random.shuffle(indices)\n",
        "\n",
        "        for j in indices:\n",
        "          b_new, w_new = SGD(X[j], Y[j], b, w, num_feat, num_sample,  alpha=alpha)\n",
        "          b = b_new\n",
        "          w = w_new\n",
        "\n",
        "        if i % 1000 == 0:\n",
        "          print('{}: b = {}, w = {}'.format(i, np.round(b_new, 2), np.round(w_new, 2)))\n",
        "\n",
        "    print('final: b = {}, w = {}'.format(np.round(b,2), np.round(w, 2)))\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00c905de",
      "metadata": {
        "id": "00c905de"
      },
      "source": [
        "> *Let's explore!*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "c3d839f9",
      "metadata": {
        "id": "c3d839f9",
        "outputId": "d4ab29b0-da1b-46fe-e908-bd3792af8dfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: b = -0.06, w = [-0.2  -0.07  0.54  0.14 -0.01]\n",
            "1000: b = 0.23, w = [2.73 0.9  2.38 0.51 1.17]\n",
            "2000: b = 0.54, w = [2.75 0.9  2.42 0.49 1.18]\n",
            "3000: b = 0.83, w = [2.74 0.89 2.45 0.5  1.17]\n",
            "4000: b = 1.09, w = [2.73 0.89 2.48 0.51 1.17]\n",
            "final: b = 1.33, w = [2.72 0.89 2.51 0.51 1.16]\n"
          ]
        }
      ],
      "source": [
        "SGD_train(X, Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da6db121",
      "metadata": {
        "id": "da6db121"
      },
      "source": [
        "<img src=\"https://i.pinimg.com/736x/2e/aa/7d/2eaa7d5021ca7c3c98bc93b98b9646fe.jpg\" align=\"left\" width=\"70\" height=\"70\" align=\"left\">\n",
        "\n",
        " ## Task 1: Training & Testing data\n",
        ">  Q1. In order to analyze large dataset efficiently, we will use the package `scikit-learn` to implement regression models.\n",
        ">> **Step 1**: Download the package `!pip install sklearn` \\\n",
        ">> **Step 2**: Import models ` from sklearn.linear_model import LinearRegression`\\\n",
        ">> **Step 3**: Call the module `lr = LinearRegression()` \\\n",
        ">> **Step 4**: Fit the dataset using `lr.fit({input}, {output})` and check the intercept and the coefficients using `lr.intercept_` and `lr.coef_`\n",
        "\n",
        "> More information about the package is available at: https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares\n",
        "\n",
        "> Q2. Compare the results with our findings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "b7f0c1da",
      "metadata": {
        "id": "b7f0c1da",
        "outputId": "7b2c7e42-a14b-490c-c4ad-35331bda4417",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.660912679523795\n",
            "[2.64898059 0.86619805 2.73710623 0.55868311 1.11434095]\n"
          ]
        }
      ],
      "source": [
        "################################### YOUR CODE HERE #############################\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lr = LinearRegression()\n",
        "\n",
        "model = lr.fit(X, Y)\n",
        "\n",
        "print(model.intercept_)\n",
        "print(model.coef_)\n",
        "\n",
        "###############################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.pinimg.com/736x/2e/aa/7d/2eaa7d5021ca7c3c98bc93b98b9646fe.jpg\" align=\"left\" width=\"70\" height=\"70\" align=\"left\">\n",
        "\n",
        " ## Task 2: Training & Testing data using `Linear Regression`\n",
        ">  Q3. Let's use the `data` and fit a `linear regression` model (DV = `domain1_score`.\n",
        "\n",
        "> Q4. Evaluate the R2-score from `from sklearn.metrics import r2_score`"
      ],
      "metadata": {
        "id": "_5_5AQI_VjV-"
      },
      "id": "_5_5AQI_VjV-"
    },
    {
      "cell_type": "code",
      "source": [
        "# data= pd.read_excel('./data/features.xlsx')\n",
        "data = data.dropna()\n",
        "X = data.drop(columns=['TextID','domain1_score', 'domain2_score', 'essay_id', 'essay_set'])\n",
        "Y = data.domain1_score\n",
        "\n",
        "################################### YOUR CODE HERE #############################\n",
        "model = lr.fit(X, Y)\n",
        "\n",
        "print(model.intercept_)\n",
        "print(model.coef_)\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "y_hat = lr.predict(X)\n",
        "r2_score(Y, y_hat)\n",
        "###############################################################################"
      ],
      "metadata": {
        "id": "PQAsFEEUVjnu",
        "outputId": "30a6e468-df15-4a13-d7b7-37fa2fa7581e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "PQAsFEEUVjnu",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "196.6205276062531\n",
            "[ 1.42873156e-10  7.68276083e-03  7.18749660e-04  7.68276019e-03\n",
            "  1.96876480e-11  2.16594319e-01 -9.55663468e-03  4.98671734e+00\n",
            "  3.80727051e-02  5.78376159e-01  2.12737779e-01  7.96779905e+00\n",
            " -3.55662585e-03  1.26331551e+01 -9.06477545e-04  1.25298388e+01\n",
            "  2.65720482e-03 -1.57279764e+01 -1.55238790e-03 -8.54094317e+00\n",
            "  2.08599378e-04  3.29594691e+00  4.47423888e-03 -8.89213182e+00\n",
            " -2.19739583e-04  3.26256983e+00  7.63296159e-04  6.79577697e+00\n",
            "  1.09928495e+01 -4.84666736e-01  5.59817983e-02  1.53950963e+01\n",
            "  1.00861040e+01  3.79960845e+01 -1.65538687e-01  5.04322039e+01\n",
            "  3.25834855e-02 -1.60702898e-01  5.85594450e-02  1.21502522e+01\n",
            "  5.25479095e-01  1.64709895e+01 -7.37637325e-01 -2.00530842e-11\n",
            " -3.25838176e-11  2.55977132e+01 -2.06234307e+00 -4.60397197e+00\n",
            " -2.90330236e+00 -2.83009806e-02 -5.40364816e-02 -4.58945821e-02\n",
            "  2.16618775e-01  2.31341813e-01 -5.86442131e-01  1.17778186e-01\n",
            " -4.92383253e-03 -7.70509322e-01 -3.60038887e-03 -6.87745232e-03\n",
            " -1.23086023e-01 -8.08675721e-03 -4.14101970e-02  7.76536821e+00\n",
            "  1.81171976e+00  4.24697744e+00 -6.38128207e+00 -1.99046805e+01\n",
            "  8.83840197e-01 -1.37900284e+00 -1.33445844e+01 -2.63167945e+01\n",
            " -3.79105326e-01  1.71059336e+01 -4.30779803e+01 -9.08647933e-04\n",
            "  3.97589561e-04 -4.59047388e-03  1.57931186e-03 -3.60235003e-02\n",
            " -3.10356890e-02 -2.15664210e-03  6.29391711e-04 -6.16583783e-03\n",
            "  5.13895287e-03 -2.35454531e-03 -2.25050237e-02  1.52475320e-02\n",
            "  4.81459757e-02  5.23770203e-02 -1.80453583e-03 -6.27343612e-03\n",
            " -5.93207807e-03  6.81872317e+00  8.85324606e+00 -1.71781282e+00\n",
            "  5.00857601e-02 -7.95244950e-02 -1.33900956e-01 -1.34063419e-01\n",
            " -1.52014059e-01 -2.29764007e+00  3.63810476e-01  4.90346599e-01\n",
            " -1.46934051e+00 -1.43306401e-02  2.57636540e-01 -2.68486391e-01]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6044758644232586"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.pinimg.com/736x/2e/aa/7d/2eaa7d5021ca7c3c98bc93b98b9646fe.jpg\" align=\"left\" width=\"70\" height=\"70\" align=\"left\">\n",
        "\n",
        " ## Task 3: Training & Testing data with `Logistic Regression`\n",
        ">  Q3. Let's use the `data` and fit a `logistic regression` model (DV = `domain1_score`. (Hint: You should create a ____ output). Use `from sklearn.linear_model import LogisticRegression`\n",
        "\n",
        "> Q4. Evaluate the accuracy from `from sklearn.metrics import accuracy_score`"
      ],
      "metadata": {
        "id": "NHd5dD96WA0o"
      },
      "id": "NHd5dD96WA0o"
    },
    {
      "cell_type": "code",
      "source": [
        "data= pd.read_excel('./data/features.xlsx')\n",
        "X = data.drop(columns=['TextID','domain1_score', 'domain2_score', 'essay_id', 'essay_set'])\n",
        "y = data.domain1_score\n",
        "\n",
        "################################### YOUR CODE HERE #############################\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "\n",
        "###############################################################################"
      ],
      "metadata": {
        "id": "qRelzBd1WX_M"
      },
      "id": "qRelzBd1WX_M",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}