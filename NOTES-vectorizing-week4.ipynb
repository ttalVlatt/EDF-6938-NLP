{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "530144c4",
      "metadata": {
        "id": "530144c4"
      },
      "source": [
        "---\n",
        "<h1><center>  lab 4 : Text Vectorization - Part 1 </center>\n",
        "    \n",
        "<img src=\"https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/assets/atap_0401.png\" width=\"500\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3c7066e",
      "metadata": {
        "id": "c3c7066e"
      },
      "source": [
        ">```Created by Jinnie Shin (jinnie.shin@coe.ufl.edu)```\\\n",
        ">```Date: ```\n",
        "---\n",
        "\n",
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQmNf86oJnfhpkPA9LnrFnAbfwF2VywPYpB_w&usqp=CAU\" align=\"left\" width=\"70\" height=\"70\" align=\"left\">\n",
        "\n",
        "### Required Packages or Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "02eac3c7",
      "metadata": {
        "id": "02eac3c7",
        "outputId": "b1dc428c-7db4-4c6c-d725-af08621f6abd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#!pip install nbimporter #run this if you run into an error (download nbimporter)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download(['punkt', 'wordnet', 'omw-1.4'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98aabe31",
      "metadata": {
        "id": "98aabe31"
      },
      "source": [
        "\n",
        "## **REVIEW**: Dataset\n",
        "\n",
        "> Using the text_normalizer function we created last time, we will import `essay set 2`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3687eefb",
      "metadata": {
        "id": "3687eefb"
      },
      "outputs": [],
      "source": [
        "## Think of classes like sets of functions\n",
        "\n",
        "## Can also apply each function individually\n",
        "\n",
        "## This function loops through all the normalizations procedures from week 3\n",
        "\n",
        "class text_normalize():\n",
        "\n",
        "    def import_data(self, essay_set=3):\n",
        "        import numpy as np\n",
        "        import pandas as pd\n",
        "        self.essay_set = essay_set\n",
        "        self.data = []\n",
        "        self.score = []\n",
        "        filename = 'training_set_rel3.xlsx'\n",
        "        data = pd.read_excel(filename)\n",
        "        self.data  = data[data['essay_set']==self.essay_set].essay.values\n",
        "        self.score = data[data['essay_set']==self.essay_set].domain1_score.values\n",
        "\n",
        "        return [' '.join(i) for i in self.data], pd.DataFrame(list(zip(self.data, self.score)), columns=['response', 'score'])\n",
        "\n",
        "    def cleaning(self, string):\n",
        "        import re\n",
        "        string  = string.lower() # step 1. lowercase\n",
        "\n",
        "        punc = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~' # step 2. remove punctuations\n",
        "        string  = string.strip(punc)\n",
        "\n",
        "        string  = string.replace(\"can't\", 'cannot') # step 3. replace abbreviated forms\n",
        "        string = string.replace(\"n't\", ' not')\n",
        "        string  = string.replace(\"'ll\", ' will')\n",
        "        string  = string.replace(\"'m\", ' am')\n",
        "        string = string.replace(\"he's\", \"he is\")\n",
        "        string = string.replace(\"it's\", 'it is')\n",
        "\n",
        "        return re.sub(r\"\\d+\", \"<DIGIT>\", string) # step 4. replace all the numbers to <DIGIT>\n",
        "\n",
        "    def sent_tokenize(self, string):\n",
        "        import nltk\n",
        "        return nltk.sent_tokenize(string)\n",
        "\n",
        "    def tokenize(self, string):\n",
        "        import nltk\n",
        "        return nltk.word_tokenize(string)\n",
        "\n",
        "    def stemming(self, tokens):\n",
        "        import nltk\n",
        "        stemmer = nltk.stem.PorterStemmer()\n",
        "        return [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    def lemmatize(self, tokens):\n",
        "        import nltk\n",
        "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "        return [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    def preprocess(self, essay_set=1):\n",
        "        _, self.df = self.import_data(essay_set=essay_set)\n",
        "        self.df['response'] = self.df.response.apply(self.cleaning)\n",
        "        self.df['tokens'] = self.df.response.apply(self.tokenize)\n",
        "        self.df['sentences'] = self.df.response.apply(self.sent_tokenize)\n",
        "        self.df['stem'] = self.df.tokens.apply(self.stemming)\n",
        "        self.df['lemma'] = self.df.tokens.apply(self.lemmatize)\n",
        "\n",
        "        return self.df\n",
        "\n",
        "normalizer = text_normalize() # call the text_normalize function and name it normalizer\n",
        "\n",
        "df = normalizer.preprocess(3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "fAW0XRW4ibjv",
        "outputId": "a4d33c32-f8c0-4d69-8a9b-624389ddaed8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "id": "fAW0XRW4ibjv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               response  score  \\\n",
              "0     The features of the setting affect the cyclist...    1.0   \n",
              "1     The features of the setting affected the cycli...    2.0   \n",
              "2     Everyone travels to unfamiliar places. Sometim...    1.0   \n",
              "3     I believe the features of the cyclist affected...    1.0   \n",
              "4     The setting effects the cyclist because of the...    2.0   \n",
              "...                                                 ...    ...   \n",
              "1721  In the story, the setting affected the cyclist...    2.0   \n",
              "1722  The features of the setting affect the cyclist...    1.0   \n",
              "1723  The setting greatly affects the cyclist trying...    2.0   \n",
              "1724  The features of the setting affected the cycli...    2.0   \n",
              "1725  The features of the setting in “Rough Road Ahe...    3.0   \n",
              "\n",
              "                                                 tokens  \\\n",
              "0     [The, features, of, the, setting, affect, the,...   \n",
              "1     [The, features, of, the, setting, affected, th...   \n",
              "2     [Everyone, travels, to, unfamiliar, places, .,...   \n",
              "3     [I, believe, the, features, of, the, cyclist, ...   \n",
              "4     [The, setting, effects, the, cyclist, because,...   \n",
              "...                                                 ...   \n",
              "1721  [In, the, story, ,, the, setting, affected, th...   \n",
              "1722  [The, features, of, the, setting, affect, the,...   \n",
              "1723  [The, setting, greatly, affects, the, cyclist,...   \n",
              "1724  [The, features, of, the, setting, affected, th...   \n",
              "1725  [The, features, of, the, setting, in, “, Rough...   \n",
              "\n",
              "                                              sentences  \\\n",
              "0     [The features of the setting affect the cyclis...   \n",
              "1     [The features of the setting affected the cycl...   \n",
              "2     [Everyone travels to unfamiliar places., Somet...   \n",
              "3     [I believe the features of the cyclist affecte...   \n",
              "4     [The setting effects the cyclist because of th...   \n",
              "...                                                 ...   \n",
              "1721  [In the story, the setting affected the cyclis...   \n",
              "1722  [The features of the setting affect the cyclis...   \n",
              "1723  [The setting greatly affects the cyclist tryin...   \n",
              "1724  [The features of the setting affected the cycl...   \n",
              "1725  [The features of the setting in “Rough Road Ah...   \n",
              "\n",
              "                                                   stem  \\\n",
              "0     [the, featur, of, the, set, affect, the, cycli...   \n",
              "1     [the, featur, of, the, set, affect, the, cycli...   \n",
              "2     [everyon, travel, to, unfamiliar, place, ., so...   \n",
              "3     [i, believ, the, featur, of, the, cyclist, aff...   \n",
              "4     [the, set, effect, the, cyclist, becaus, of, t...   \n",
              "...                                                 ...   \n",
              "1721  [in, the, stori, ,, the, set, affect, the, cyc...   \n",
              "1722  [the, featur, of, the, set, affect, the, cycli...   \n",
              "1723  [the, set, greatli, affect, the, cyclist, tri,...   \n",
              "1724  [the, featur, of, the, set, affect, the, cycli...   \n",
              "1725  [the, featur, of, the, set, in, “, rough, road...   \n",
              "\n",
              "                                                  lemma  \n",
              "0     [The, feature, of, the, setting, affect, the, ...  \n",
              "1     [The, feature, of, the, setting, affected, the...  \n",
              "2     [Everyone, travel, to, unfamiliar, place, ., S...  \n",
              "3     [I, believe, the, feature, of, the, cyclist, a...  \n",
              "4     [The, setting, effect, the, cyclist, because, ...  \n",
              "...                                                 ...  \n",
              "1721  [In, the, story, ,, the, setting, affected, th...  \n",
              "1722  [The, feature, of, the, setting, affect, the, ...  \n",
              "1723  [The, setting, greatly, affect, the, cyclist, ...  \n",
              "1724  [The, feature, of, the, setting, affected, the...  \n",
              "1725  [The, feature, of, the, setting, in, “, Rough,...  \n",
              "\n",
              "[1726 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cfb23aae-6aaf-48a3-87ee-d61319940ba3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>response</th>\n",
              "      <th>score</th>\n",
              "      <th>tokens</th>\n",
              "      <th>sentences</th>\n",
              "      <th>stem</th>\n",
              "      <th>lemma</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The features of the setting affect the cyclist...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[The, features, of, the, setting, affect, the,...</td>\n",
              "      <td>[The features of the setting affect the cyclis...</td>\n",
              "      <td>[the, featur, of, the, set, affect, the, cycli...</td>\n",
              "      <td>[The, feature, of, the, setting, affect, the, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The features of the setting affected the cycli...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>[The, features, of, the, setting, affected, th...</td>\n",
              "      <td>[The features of the setting affected the cycl...</td>\n",
              "      <td>[the, featur, of, the, set, affect, the, cycli...</td>\n",
              "      <td>[The, feature, of, the, setting, affected, the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Everyone travels to unfamiliar places. Sometim...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[Everyone, travels, to, unfamiliar, places, .,...</td>\n",
              "      <td>[Everyone travels to unfamiliar places., Somet...</td>\n",
              "      <td>[everyon, travel, to, unfamiliar, place, ., so...</td>\n",
              "      <td>[Everyone, travel, to, unfamiliar, place, ., S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I believe the features of the cyclist affected...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[I, believe, the, features, of, the, cyclist, ...</td>\n",
              "      <td>[I believe the features of the cyclist affecte...</td>\n",
              "      <td>[i, believ, the, featur, of, the, cyclist, aff...</td>\n",
              "      <td>[I, believe, the, feature, of, the, cyclist, a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The setting effects the cyclist because of the...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>[The, setting, effects, the, cyclist, because,...</td>\n",
              "      <td>[The setting effects the cyclist because of th...</td>\n",
              "      <td>[the, set, effect, the, cyclist, becaus, of, t...</td>\n",
              "      <td>[The, setting, effect, the, cyclist, because, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1721</th>\n",
              "      <td>In the story, the setting affected the cyclist...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>[In, the, story, ,, the, setting, affected, th...</td>\n",
              "      <td>[In the story, the setting affected the cyclis...</td>\n",
              "      <td>[in, the, stori, ,, the, set, affect, the, cyc...</td>\n",
              "      <td>[In, the, story, ,, the, setting, affected, th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1722</th>\n",
              "      <td>The features of the setting affect the cyclist...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[The, features, of, the, setting, affect, the,...</td>\n",
              "      <td>[The features of the setting affect the cyclis...</td>\n",
              "      <td>[the, featur, of, the, set, affect, the, cycli...</td>\n",
              "      <td>[The, feature, of, the, setting, affect, the, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1723</th>\n",
              "      <td>The setting greatly affects the cyclist trying...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>[The, setting, greatly, affects, the, cyclist,...</td>\n",
              "      <td>[The setting greatly affects the cyclist tryin...</td>\n",
              "      <td>[the, set, greatli, affect, the, cyclist, tri,...</td>\n",
              "      <td>[The, setting, greatly, affect, the, cyclist, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1724</th>\n",
              "      <td>The features of the setting affected the cycli...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>[The, features, of, the, setting, affected, th...</td>\n",
              "      <td>[The features of the setting affected the cycl...</td>\n",
              "      <td>[the, featur, of, the, set, affect, the, cycli...</td>\n",
              "      <td>[The, feature, of, the, setting, affected, the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1725</th>\n",
              "      <td>The features of the setting in “Rough Road Ahe...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>[The, features, of, the, setting, in, “, Rough...</td>\n",
              "      <td>[The features of the setting in “Rough Road Ah...</td>\n",
              "      <td>[the, featur, of, the, set, in, “, rough, road...</td>\n",
              "      <td>[The, feature, of, the, setting, in, “, Rough,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1726 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cfb23aae-6aaf-48a3-87ee-d61319940ba3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cfb23aae-6aaf-48a3-87ee-d61319940ba3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cfb23aae-6aaf-48a3-87ee-d61319940ba3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d75c156",
      "metadata": {
        "id": "3d75c156"
      },
      "source": [
        "#### Essay Set 2\n",
        "`Prompt`: A student performed the following investigation to test four different polymer plastics for stretchability.\n",
        "`Procedure`:\n",
        "> 1. Take a sample of one type of plastic, and measure its length.\n",
        "> 2. Tape the top edge of the plastic sample to a table so that it is hanging freely down the side of the table.\n",
        "> 3. Attach a clamp to the bottom edge of the plastic sample.\n",
        "> 4. Add weights to the clamp and allow them to hang for five minutes.\n",
        "> 5. Remove the weights and clamp, and measure the length of the plastic types.\n",
        "> 6. Repeat the procedure exactly for the remaining three plastic samples.\n",
        "> 7. Perform a second trial (T2) exactly like the first trial (T1).\n",
        "\n",
        "The student recorded the following data from the investigation.\n",
        "\n",
        "`Data Table`:\n",
        "\n",
        "\n",
        "|           Plastic Type    |     Amount Stretched     (mm)- T1    |   Amount Stretched     (mm)- T2         |\n",
        "|---------------------------|----------------------------------|-----------|\n",
        "|     A                     |     10                           |     12    |\n",
        "|     B                     |     22                           |     23    |\n",
        "|     C                     |     14                           |     13    |\n",
        "|     D                     |     20                           |     20    |\n",
        "\n",
        "> a.\tDraw a conclusion based on the student’s data. \\\n",
        "> b.\tDescribe two ways the student could have improved the experimental design and/or validity of the results.\n",
        "\n",
        "\n",
        "\n",
        "| Type of response            | Source dependent response |\n",
        "|-----------------------------|---------------------------|\n",
        "| Grade level                 | `10`                       |\n",
        "| Subject                     | `Science`                   |\n",
        "| Total sample size           | `1,783`                     |\n",
        "| Average length of responses | `50 words`                  |\n",
        "| Score range                 | `0-3`                       |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66fb145d",
      "metadata": {
        "id": "66fb145d"
      },
      "source": [
        "\n",
        "## 1. Text Vectorization\n",
        "### 1.1 Term-Document Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "70d8f0e0",
      "metadata": {
        "id": "70d8f0e0",
        "outputId": "34407e7e-e3e2-4bc1-9dcf-894f9ca36aab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({4: 4, 3: 3, 2: 2, 1: 1})\n",
            "Counter({'cherry': 5, 'pear': 3, 'apple': 2})\n"
          ]
        }
      ],
      "source": [
        "# Our goal is to use the output we created above (\"cleaned\" column) to create a term-document matrix\n",
        "# Let's take a look at this additional function we can use \"Counter\"\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "print(Counter([1,2,2,3,3,3,4,4,4,4])) # Counts the unique element in the given list\n",
        "\n",
        "print(Counter(['apple', 'apple', 'pear', 'pear', 'pear']+['cherry']*5))\n",
        "\n",
        "## Creates dictionary like output with tokens and counts\n",
        "\n",
        "Counter(df['tokens'][1])\n",
        "\n",
        "## Can turn into acutal dictionary with dict()\n",
        "\n",
        "dictionary = dict(Counter(df['tokens'][1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3567d4ac",
      "metadata": {
        "id": "3567d4ac"
      },
      "source": [
        "> Let's create a function called `count_vectorizer`, which takes a dataframe (`data`) as an input and produce `bow` and `unique_vocabulary` as the outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9e7f06f0",
      "metadata": {
        "id": "9e7f06f0",
        "outputId": "cb6b7399-4318-4049-afc9-22af136a6d89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             0     1     2     3     4     5     6     7     8     9     ...  \\\n",
            "the           8.0  19.0   5.0   8.0  17.0   7.0   3.0   6.0   8.0  11.0  ...   \n",
            "featur        2.0   1.0   0.0   1.0   0.0   3.0   0.0   0.0   0.0   0.0  ...   \n",
            "of            4.0   4.0   0.0   6.0   8.0   3.0   0.0   5.0   2.0   2.0  ...   \n",
            "set           2.0   1.0   1.0   0.0   4.0   2.0   0.0   0.0   2.0   0.0  ...   \n",
            "affect        2.0   2.0   1.0   1.0   0.0   1.0   0.0   3.0   1.0   2.0  ...   \n",
            "...           ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
            "california…   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
            "evidenc       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
            "goup          0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
            "hills…        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
            "seveal        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
            "\n",
            "             1716  1717  1718  1719  1720  1721  1722  1723  1724  1725  \n",
            "the          10.0   9.0   0.0  12.0  14.0   7.0   4.0  10.0  13.0  11.0  \n",
            "featur        1.0   0.0   0.0   3.0   1.0   0.0   1.0   0.0   1.0   1.0  \n",
            "of            7.0   2.0   1.0   4.0   3.0   2.0   2.0   4.0   2.0   4.0  \n",
            "set           1.0   0.0   0.0   3.0   1.0   1.0   1.0   2.0   2.0   2.0  \n",
            "affect        2.0   0.0   0.0   2.0   1.0   1.0   1.0   3.0   2.0   1.0  \n",
            "...           ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
            "california…   0.0   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0  \n",
            "evidenc       0.0   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0  \n",
            "goup          0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0   0.0   0.0  \n",
            "hills…        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0  \n",
            "seveal        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0  \n",
            "\n",
            "[5095 rows x 1726 columns]\n",
            "['the', 'featur', 'of', 'set', 'affect', 'cyclist', 'in', 'mani', 'way', '.', 'that', 'is', 'lact', 'inform', 'on', 'were', 'to', 'go', 'and', 'lack', 'water', 'thi', 'wa', 'a', 'problem', 'becaus', 'he', 'need', 'for', 'hi', 'trip', 'direct', 'neg', 'desert', ',', 'which', 'dri', 'hot', '“', 'i', 'travel', 'through', 'high', 'california', 'june', '``', '(', 'kurmaski', '@', 'num1', ')', 'also', 'temperatur', 'made', 'everi', 'thing', 'around', 'him', 'brackish', 'fale', 'somewher', 'neighborhood', 'two', 'hundr', 'degre', \"''\", 'sinc', 'there', 'no', 'escap', 'from', 'sun', 'text', 'state', 'begin', 'beat', 'down', '”', 'num3', 'all', 'alon', 'road', 'with', 'none', 'either', 'so', 'into', 'pass', 'out', 'one', 'could', 'help', 'grow', 'realiz', 'drop', 'heatstrok', 'gorgou', 'day', 'about', 'forti', 'mile', 'pedal', 'arriv', 'at', 'first', '‘', 'town', '’', 'but', 'morn', 'it', 'fit', 'tradit', 'definit', 'ghost', 'theset', 'ride', \"n't\", 'good', 'they', 'negativli', ';', 'caus', 'be', 'dehydr', 'tiredin', 'end', 'everyon', 'unfamiliar', 'place', 'sometim', 'we', 'get', 'lost', 'ask', 'local', 'month1', 'not', 'idea', 'had', 'perfectli', 'map', 'older', 'men', 'who', 'look', 'like', 'haven', 't', 'been', 'age', 'old', 'appar', 'gave', 'wrong', 'my', 'advic', 'would', 'listen', 'anyon', 'matter', 'their', 'if', 'you', 'have', 'tri', 'know', 'where', 'are', 'time', 'ye', 'sir', '!', 'believ', 'impati', 'trustworthi', 'tock', 'take', 'an', 'hr', 'bay', 'short', 'cat', 'stay', 'true', 'mep', 'wouldn', 'almost', 'heat', 'impatientress', 'got', 'best', 'took', 'shortcut', 'after', 'exper', 'new', 'hae', 'stat', '?', 'trust', 'peopl', 'havent', 'seen', 'chang', 'last', 'few', 'year', 'effect', 'diffrent', 'stori', 'sens', 'scaterd', 'when', 'reach', 'see', 'ramshackl', 'shed', 'sever', 'rusti', 'pump', 'coral', 'couldnt', 'hold', 'lamesh', 'mule.', 'conflict', 'speeker', 'say', 'travl', 'veri', 'sete', 'forest', 'date1', 'as', 'much', 'less', 'excit', 'instanc', 'extrem', 'nearli', 'cross', '.a', 'her', 'ram', 'shackl', 'corral', 'caldnt', 'hald', 'lamest', 'move.an', 'give', 'haw', 'feel', 'tower', 'stop', 'these', 'told', 'cut', 'did', 'conclus', 'doe', 'hurt', 'lot', 'caps1', 'don', 'care', 'm', 'them', 'me', 'or', 'big', 'deal', 'write', 'some', 'blow', 'stuff', 'up', 'talk', 'tiv', 'radio', 'make', 'black', 'ha', 'anyth', 'us', 'realli', 'essay', 'rough', 'ahead', ':', 'do', 'exce', 'post', 'speed', 'limit', 'describ', 'man', 's', 'bicycl', 'now', 'dure', 'summer', 'greatli', 'journey', 'difficult', 'finish', 'drank', 'most', 'bottl', 'contain', 'onli', 'tantal', 'sips.', 'can', 'bikerid', 'hard', 'written', 'by', 'joe', 'repres', 'how', 'certain', 'condit', 'bike', 'yosemit', 'nation', 'park', 'accout', 'tough', 'exampl', 'down.', 'ani', 'task', 'drain', 'person', 'energi', 'secondli', 'long', 'conserv', 'ran', 'death', 'environ', 'author', 'perform', 'strength', 'tremend', 'whether', 'hill', 'barren', 'landscap', 'three', 'toll', 'deplet', 'well.', 'flat', 'repalc', 'roll', 'hills.', 'overcam', 'exhaust', 'build', 'came', 'views.', 'wipe', 'sweat', 'eye', 'sure', 'wasn', 'mirag', 'too', 'what', 'burst', 'manouv', 'hill.', 'authour', 'ne', 'face', 'even', 'though', 'seem', 'piec', 'exaggr', 'import', 'keep', 'want', 'winston', 'churchil', 'onc', 'said', 'success', 'final', 'failur', 'fatal', 'courag', 'continu', 'counts.', 'person2', 'agre', 'mean', 'cold', 'hydrat', 'longer', 'more', 'will', 'rain', 'snow', 'possibl', 'slow', 'muscl', 'might', 'stiff', 'abl', 'move', 'fast', 'whi', 'such', 'drink', 'throughout', 'littl', 'left', 'eas', 'past', 'balanc', 'state.', 'show', 'prepar', 'enough', 'without', 'he/sh', 'die', 'wide', 'ring', 'circl', 'shirt', 'stroke', 'danger', 'system', 'him/her', 'lose', 'initi', 'went', 'snake', 'middl', 'cation', 'uneas', 'furthermor', 'noth', 'abando', 'troubl', 'reliev', 'bait', 'store', 'kurmaksi', 'put', 'hardship', 'probabl', 'avoided.th', 'cotag', 'path', 'thought', 'right', 'place.th', 'replac', 'use', 'same', 'dessert', 'scenari', 'wahil', 'scene', 'hour', 'start', 'tire', 'bore', 'everyth', 'drossi', 'fountain', 'faint', 'differ', 'characterist', 'determin', 'outcom', 'event', 'im', 'weather', 'reloud', 'case', 'deten', 'read', 'maximum', 'level', 'abil', 'your', 'labe', 'lastli', 'type', 'amount', 'ground', 'cover', 'overal', 'factor', 'life', 'alway', 'constant', 'thirst', 'addit', 'area', 'appear', 'quit', 'desol', 'find', 'abondor', 'car', 'sign', 'modern', 'civil', 'unnerv', 'someth', 'happen', 'nobodi', 'especi', 'prevel', 'becom', 'those', 'person1', 'than', 'befor', 'hit', 'pretti', 'regularli', 'cycl', 'awar', 'follow', 'codger', 'didnt', 'next', 'tarlik', 'substanc', 'ooz', 'she', 'thoot', 'then', 'terrain', 'wasnt', 'better', 'tumbl', 'weed', 'ridicul', 'larg', 'block', 'date', 'empti', 'coupl', 'race', 'wind', 'traffic', 'law', 'side', 'between', 'harzard', 'defi', 'endur', 'pain', 'given', 'bad', 'claim', 'num1.also', 'ration', 'our', 'charact', 'natur', 'skill', 'd', 'learn', 'began', 'suck', 'pebbl', 'mind', 'off', 'saliva', 'refresh', 'throat', 'adventur', 'kept', 'young', 'whenev', 'faster', 'surviv', 'ever', 'again', 'lock', 'just', 'eliterat', 'frind', 'e', 'tar', 'come', 'tast', 'batteri', 'couldn', 'lean', 'gete', 'becam', 'cooler', 'handl', 'rust', 'felt', 'top', 'els', 'watch', 'rock', 'confid', 'accept', 'doesn', 'suspect', 'suppli', 'land', 'mark', 'exactli', 'suppos', 'replenish', 'diminish', 'howev', 'further', 'press', 'sceneri', 'drastic', 'hotter-', 'tumblewe', 'despit', 'along', 'juic', 'shop', 'cang', 'send', 'worri', 'hopeless', 'relief', 'well', 'upset', 'easi', 'angr', 'stress', 'pysic', 'mental', 'wiredi', 'wear', 'physic', 'inclin', 'steep', 'against', 'graviti', 'egaust', 'crusal', 'sauna', 'attract', 'byciclist', 'instintani', 'think', 'death.', 'dead', 'anim', 'bone', 'vultur', 'afraid', 'wors', 'grvele', 'while', 'due', 'thu', 'experienc', 'major', 'luckili', 'camp', 'found', 'fresh', 'food', 'eventu', 'across', 'guy', 'never', 'leav', 'porch', 'posit', 'later', 'turn', 'exist', 'anoth', 'wast', 'desert-lik', 'harder', 'result', 'rehydr', 'himself', 'common', 'knowledg', 'low', 'mouth', 'caps2', 'wild', 'diamondbrock', 'venom', 'chanc', 'writer', 'worn', 'thirsti', 'wore', 'factori', 'pictur', 'cup', 'reason', 'mshan', 'rode', 'timit', 'overcom', 'gole', 'understand', 'realit', 'mode-al', 'book', 'underst', 'today', 'wont', 'thrown', 'mode', 'usual', 'chase', 'lead', 'hilli', 'rather', 'sooth', 'strong', 'leg', 'smile', 'face.', 'energ', 'happi', 'reader', 'predict', 'remain', 'joy', 'basic', 'experi', 'bodi', 'cruel', 'emphas', 'mood', 'enthusiast', 'forlorn', 'connect', 'real', 'obstacl', 'includ', 'must', 'persever', 'great', 'pine', 'tree', 'cool', 'pour', 'away', 'point', 'refil', 'brown', 'didn', 'plu', 'form', 'giant', 'potentiali', 'kill', 'narrat', 'goe', 'insist', 'hope', 'challeng', 'despair', 'develop', 'campsit', 'anywher', 'gotten', 'reliabl', 'apart', 'symbol', 'decis', 'perhap', 'alreadi', 'stranger', 'yet', 'should', 'ironi', 'weren', 'healthi', 'necessari', 'plot', 'line', 'humour', 'possess', 'citi', 'strand', 'therefor', 'river', 'ant', 'pound', 'still', 'sleep', 'wake', 'pull', 'plase', 'crazi', 'fix', 'hous', 'fall', '..', 'bunch', 'inconclus', 'doesnt', 'power', 'offer', 'discourag', 'biker', 'elderli', 'notic', 'sight', 'structur', 'kind.', 'tone', 'word', 'intens', 'inth', 'crippl', 'rideeasi', 'shortag', 'tall', 'fine', 'alot', 'bumpi', 'rood', 'work', 'abandon', 'human', 'cyclist.th', 'excited.', 'outsid', 'avail', 'over', 'rougher', 'distanc', 'until', 'consum', 'ehaust', 'divid', 'approach', 'increas', 'soon', 'fade', 'unabl', 'acid', 'tackl', 'recov', 'exceed', 'mainli', 'season', 'll', 'bring', 'botll', 'nextim', 'bump', 'aror', 'react', 'humor', 'chuckl', 'check', 'safe', 'fork', 'grew', 'saw', 'near', 'caught', 'desper', 'devast', 'plan', 'bird', 'pick', 'clean', 'fish', 'overwhelm', 'urg', 'seek', 'gari', 'wilber', 'kiss', 'buy', 'bait.', 'overjoy', 'invit', 'cycli', 'trun', 'surround', 'warm', 'bright', 'express', 'play', 'speak', 'remors', 'led', 'shortcut.', 'let-down', 'male.', 'battery-acid', 'flavor', 'forc', 'onward', 'piti', 'anger', 'predica', '&', 'sink', 'full', 'organization1', 'thank', 'light', 'dark', 'tunnel', 'limit.', 'rid', 'tontal', 'sell', 'pobabl', 'suffer', 'nowher', 'biggest', 'issu', 'location1', 'june.', 'run', 'degrees.', 'humid', 'main', 'fact', 'cooper', 'stone', 'solo', 'encount', 'directli', 'complet', 'health', 'acknowledg', 'deserv', 'hute', '...', 'disappoint', 'truck', 'adequ', 'concumpt', 'link', 'elsewher', 'quench', 'explenc', 'exsercis', 'spend', 'quick', 'dyhydr', 'distract', 'wer', 'bate', 'unmerci', 'heath', 'creat', 'obstract', 'deter', 'goal', 'hadnt', 'live', 'creatur', 'except', 'infer', 'delicaci', 'releas', 'runningout', 'preciou', 'lone', 'inspir', 'l', 'cald', 'gorgeou', 'grape', 'servic', 'anxious', 'dispar', 'similar', 'wat', 'walk', 'freez', 'wald', 'breez', 'air', 'part', 'quot', 'timer', 'confin', 'carter', 'offic', 'terrail', 'realivingli', 'tell', 'other', 'trobl', 'fell', 'toil', 'rattl', 'honeycomb', 'detor', 'brain', 'slowli', 'impact', 'dont', 'foward', 'back', \"'s\", 'advanag', 'messag', '-', 'drive', 'quickley', 'nervou', 'model', 'scare', 'metion', 'auther', 'sound', 'both', 'cecilla', 'ssing', 'ceoo', 'goo', 'tie', 'enjoy', 'nius', 'spark', 'bor', 'counter', 'tirst', 'sact', 'sec', 'add', 'perso', 'mic', 'hou', '–timer', 'thou', 'tailor', 'save', 'shouldn', 'nidsn', 'transitin', 'califend', 'fere', 'let', 'sick', 'unstabl', 'deffinitli', 'exed', 'featutr', 'wich', 'wount', 'journey.a', 'shut', 'pot', 'spin', 'commiss', 'sad', 'deep', 'mostli', 'strive', 'rush', 'rivers…', 'bit', 'upon', 'climb', 'teas', 'destin', 'fill', 'pride', 'strenuou', 'pathway', 'mess', 'prepair', 'enuff', 'senari', 'worst', 'creep', 'pack', 'regret', 'endless', 'mercil', 'mankind', 'fluid', 'movement', 'produc', 'isn', 'easier', 'diamondback', 'perfect', 'regular', 'smooth', 'depend', 'stregth', 'far', 'attitud', 'advers', 'present', 'heatstak', 'climat', 'often', 'element', 'water-deplet', '12', 'mph', 'optimist', 'small', 'influenti', 'isol', 'fatigu', 'own', 'urban', 'ordeal', 'known', 'role', 'interest', 'opinion', 'ruin', 'dirti', '…', 'sort', 'largest', 'mule', 'greet', 'farlik', 'section', 'ridi', 'haurst', 'dry.lik', 'dry.second', 'dangerous.a', 'diamond', 'on.at', 'larger', 'skilt', 'adiamond', 'are.that', 'surrand', 'cylceist', 'terrian', 'entir', 'focu', 'cycleist', 'actual', 'refuel', 'sonni', 'rider', 'afternoon', 'wonder', 'dramat', 'unbear', 'smolter', 'impeid', 'stumb', 'snack', 'tar-lik', 'suspici', 'month', 'park.th', 'perciev', 'campground', 'dusti', 'disert', 'desiret', 'here', 'goin', '`', 'street', 'busi', 'arround', 'minut', 'stight', 'jump', 'forward', 'endus', 'swetter', 'hed', 'earli', 'endeav', 'breli', '[', ']', 'name', 'although', 'eras', 'beali', 'noticedth', 'stripe', 'bitter', 'tumblew', 'quech', 'least', 'divert', 'assert', 'spirit', 'risen', 'instead', 'popors', 'plenti', 'tune', 'passiv', 'grief', 'visual', 'hott', 'head', 'paragraph', 'five', 'adject', 'brung', 'beliv', 'miracl', 'welch', 'fruit', 'choic', 'passag', 'water.anoth', 'terrains.wher', 'laugh', 'place.both', 'pipe', 'snake.', 'kind', 'windi', 'slower', 'sald', 'someon', 'ahead…', 'pg', 'torn', 'beaten', 'manuv', 'control', 'tired', 'expos', 'sunburn', 'heart', 'each', 'wastland', 'relev', 'anti', 'excureiatli', 'physicli', 'mentali', \"'rough\", \"'\", 'influenc', 'miser', 'uneven', 'whole', 'section.', 'piney', 'lake', 'dirt', 'sourc', 'trail', 'hidden', 'beneath', 'friendli', 'critter', 'color', 'occasion', 'doubt', 'aliv', 'run-down', 'distraught', 'fortun', 'definetli', 'boost', 'assur', 'sat', 'wade', 'search', 'resevoir', 'compground', 'seren', 'early-summ', 'toward', 'yosemite.', 'beauti', 'anticip', 'funni', 'thrive', 'spot.', 'shape', 'push', 'enter', 'bathroom', 'relax', 'ga', 'station', 'money', 'num2', 'mayb', 'wherev', 'defentili', 'nice', 'concentr', 'greatest', 'oldest', 'stick', 'trusti', 'iron', 'twist', 'easlier', 'it-had', 'expect', 'halfway', 'pedel', 'tuoard', 'phisic', 'demand', 'numer', 'nother', 'complain', 'week', 'explain', 'huge', '55', 'risk', 'uncomfort', 'dehidr', 'protagonist', 'quickli', 'simpli', 'inevit', 'sand', 'blast', 'boy', 'glass', 'seen.', 'sorriest', 'unluckiest', 'world', 'craft', 'meant', 'pure', 'taunt', 'quiet', 'straight', 'cycler', 'choos', 'rout', 'loss', 'tragic', 'dehadrayt', 'tierd', 'stubl', 'deepli', 'allow', 'cylist', 'regain', 'carri', 'eassi', 'clear', 'fight', 'green', 'grass', 'mountain', 'collid', 'beam', 'different.h', 'sentenc', 'deterior', 'heatstroke.', 'lf', 'ok', 'chronic', 'imagin', 'hiss', 'aw', 'wouldnt', 'horrif', 'evid', 'excess', 'object', 'cyclsit', 'pavement', 'front', 'progress', 'hostil', 'am', 'surpris', 'lucki', 'misl', 'seneri', 'struggl', 'anymor', 'fear', 'support', 'begun', 'diamondback-block', 'me.', 'under', 'extremelyli', 'remind', '…a', 'hung', 'de', 'majorli', 'self', 'esteem', 'diy', 'explan', 'tier', 'diel', 'poison', 'town.', 'reassur', 'hottest', 'taken', 'resen', 'spot', 'time-say', 'war-but', 'statement', 'impress', 'particular', 'decad', 'order', 'fisrt', 'toun', 'cyclilst', 'aris', 'doun', 'sip', 'driead', 'heatsrok', 'gorqea', 'gentleman', 'respones', 'wasen', 'frustrat', 'angri', 'pase', 'sow', 'compani', 'hurri', 'countless', 'surfac', 'lend', 'hand', 'beg', 'ablut', 'italso', 'aas', 'descript', 'harsh', 'vivid', 'establish', 'tyer', 'crush', 'wait', 'horribl', 'sapos', 'reward', 'choke', 'luck', 'lodg', 'calm', 'nor', 'drip', 'flaver', 'beatterli', 'acid.when', 'train', 'tarrain', 'knew', 'deffin', 'hotlik', 'gorger', 'bumbi', 'cautious', 'terror', 'gon', 'na', 'rest', 'merri', \"'do\", 'demonstr', 'bu', 'hadn', 'altermm', 'ramshackel', 'sched', 'corral…', 'acid.', 'test', 'inner', 'spirit.', 'situat', 'somewhat', 'parti', 'tempetur', 'easili', 'automat', 'won', 'narrow', 'rocki', 'wildlif', 'poiso', 'infront', 'caution', 'disadvantag', 'obvious', 'bare', 'noticed.', 'beggin', 'rum', 'mirage…', 'petal', 'slipperi', 'crash', 'harm', 'ti', '.at', '.he', 'plat', 'him.', '.then', 'highway', '.i', 'faster.', '.bi', 'period', '.so', '.thi', 'sunni', 'defin', 'tration', 'forget', 'split', 'circul', 'rework', 'shade', '.all', 'touent', 'fals', 'sludg', 'approv', 'close', 'love', 'seceneri', 'decid', 'gain', 'elev', 'hope.', 'hotwher', 'bycicl', 'gone', 'hauntinghim', 'mad', 'lodger', 'hetalk', 'avoid', 'truli', 'baron', 'relentless', 'burn', 'calori', 'tortur', 'billboard', 'emot', 'contribut', 'hopless', 'vermont', 'no-brair', 'comlain', 'proven', 'rise', 'tend', 'site', 'eager', 'pire', 'farther', 'hotter', 'pebbt', 'spit', 'lugubvi', 'dire', 'relat', 'unfit', 'zoo', 'setting-rel', 'pothol', 'thug', 'reliz', 'laught', 'self.', 'refer', 'th', 'tental', 'realic', 'wise', 'youth', 'silli', 'mistak', 'soul', 'ho', 'cyclest', 'lade', 'aint', 'weaken', 'war.', 've', 'eneri', 'mislead', 'downhil', 'dryness', 'ot', 'sorround', 'sandblast', 'wish', 'ricketi', 'herself', 'weatheri', 'cloth', 'weigh', 'achiev', 'obtain', 'stabl', 'brake', 'chain', 'gloomi', 'nasti', 'behind', 'typess', 'burrow', 'combin', 'deadli', 'pray', 'restock', 'bake', 'declin', 'gentlemen', '--', 'parement', 'enform', 'acicct', 'exact', 'alik', 'strang', 'disfernd', 'slic', 'succeed', 'attack', 'unlik', 'cut.', 'page', 'senc', 'foreshadow', 'dizzi', 'loos', '…i', 'heatstroke…', 'clean…', 'receiv', 'weak', 'dust', 'lung', 'mirage.', 'difer', 'reli', 'todi', 'neck', 'cycilist', 'independ', 'elder', 'whose', 'chose', 'eighteen', 'variou', 'odd', 'june…', 'ganl', 'strain', 'manag', 'clearli', 'diamondbuck-blook', 'remot', 'sit', 'daylight', 'per', 'waterflow', 'efect', 'photograph', 'barrain', 'nde', 'act', 'cours', 'reciev', 'delin', 'ahead.', 'warri', 'saying.', 'riski', 'switch', 'says.', 'comer', 'water-', 'abanon', 'expens', 'gear', 'definatli', 'diari', 'balt', 'bathrom', 'rod', 'faetur', 'neccessari', 'misfortun', 'eight', 'infact', 'tatal', 'ropolal', 'done', 'folk', 'besid', 'weari', 'ad', 'laugh.', 'anthor', 'warn', 'lost-al', 'two-hundr', 'relif', 'hardest', 'helper', 'up-hil', 'definuntli', 'pedl', 'cautiou', 'characteric', 'imag', 'swelter', 'dilapid', 'shack', 'unfavor', 'helpless', 'favor', 'dihidr', 'shown', 'speaker', 'acid-that', 'old-tim', 'leason', 'break', 'pluk', 'refreash', 'horizon', 'myself', 'unsur', 'outdoor', 'defici', 'strongli', 'cripppl', 'maneuv', 'consid', 'difficulti', 'possit', 'wilbel', 'camp.', 'extra', 'forth', 'effort', 'accomplish', 'dream', 'tougher', 'drash', 'non', 'lodi', 'piont', 'view', \"'d\", 'themselv', 'spout', 'breezi', 'aunt.', 'futur', 'consies', 'threu', 'forwad', 'barnest', 'impos', 'threw', 'irrita', 'ten', 'unknown', 'blame', 'reali', 'motuis', 'credit', 'mindset', 'cheeri', 'denyov', 'early-', 'summer-', 'evening.', 'serr', 'peac', 'denyoy', 'unless', 'cherri', 'sceri', 'steryotyp', 'dreari', 'pessimist', 'poor', 'homeless', 'standard', 'open', 'snakes.', 'useless', 'perch', 'viewpoint', 'correct', 'exe', 'cope', 'anduou', 'enviro', 'tole', 'secont', 'bick', 'ba', 'jar', 'demon', 'bicyclist', 'home', 'free', 'bend', 'rand', 'beet', 'abbanond', 'head.', 'spote', 'shop.', 'somehow', 'clime', 'round', '–', 'bet', 'realcav', 'mention', 'impli', 'capabl', 'perpetu', 'preform', 'key', 'ingreedi', 'twice', 'quicker', 'silent', 'yell', 'togeth', 'die…', 'measur', 'shoud', 'er', 'di', 'apoint', 'ride.', 'men.', 'brave', 'obsticl', 'sigh', 'bumpli', 'snake-', 'back-block', 'bigger', 'crip', 'someplac', 'abond', 'bot', 'plant.', 'spask', 'hobbl', 'spedd', 'joke', 'mild', 'troubling.', 'knee', 'depleed', 'famili', 'morvie/video', 'throught', 'u', 'intent', 'convers', 'doger', 'clean.', 'flow', 'consern', 'die.', 'dosent', 'inde', 'tale', 'gener', 'titl', 'verg', 'articl', 'wrote', 'misadventur', 'commend', 'reckless', 'oh', 'yeah', 'scorch', 'warmer', 'thirtier', 'kinda', 'paranoid', 'number', 'size', 'fifth', 'prior', 'fifti', 'sweati', 'luer', 'everytim', 'buld', 'witch', 'plant', 'thirstier', 'wood', 'hole', 'unfortun', 'maze', 'exisit', 'wari', 'delievi', 'enorm', 'cruel.', 'patienc', 'sammer', 'exert', 'hazard', 'seriou', 'varieti', 'etc', 'learntedat', 'time-sat', 'war', 'lie…slowli', 'lern', 'lifel', 'rememb', 'current', 'earlier', 'lesson', 'augsost', 'buil', 'stuck', 'andh', 'collaps', 'steam', 'wose', 'heat.though', 'peril', 'seeth', 'par', 'dedic', 'tgive', 'all-', 'detail', 'shed…and', 'mule…', 'yosemite…', 'rapidli', 'itself', 'qualiti', 'lower', 'emphasi', 'worth', 'thout', 'lukli', 'phase', 'accord', 'familiar', 'locat', 'readi', 'serious', 'return', 'chill', 'meet', 'din', 'midd', 'aid', 'kit', 'effici', 'hyperthermia', 'sustain', 'yourself', 'presenc', 'cautionari', 'non-domest', '…rough', 'pressur', 'motiv', 'teenag', 'peddl', 'ginaupup', 'pace', 'keen', 'poster', 'child', 'thier', 'aloud', 'moist', 'opt', 'alongsid', 'process', 'drought', 'cy', 'clist', 'reboost', 'brakish', 'abov', 'below', 'liquid', 'solid', 'dump', 'signific', 'lea', 'sorri', 'myself.', 'fading.', 'stumbl', 'inconsist', 'prove', 'hottn', 'resourc', 'rug', 'select', 'date2', 'comfort', 'scarc', 'parabl', 'jaunt.', 'twenti', 'confus', 'puzzl', 'suceed', 'confiden', 'jerni', 'damag', 'cough', 'terain', 'clue', 'querur', 'confidecnc', 'insan', 'unber', 'sanili', 'terribl', 'sleneci', 'second', 'abund', 'disturb', 'trebling.', 'fin', 'brickish', 're', 'ach', 'luck.', 'buildingc', 'cabl', 'haze', 'tiresom', 'gona', 'blare', 'trechar', 'devat', 'bother', 'dehydrt', 'sport', 'jacket', 'secur', 'burb', 'badli', 'gather', 'apebbl', 'habit', 'weaker', 'water.th', 'haunt', 'nt', 'substantiti', 'arid', 'equal', 'conquer', 'findswat', 'exit', 'promis', 'mediocr', 'target', 'dwell', 'codgen', 'materi', 'utterli', 'wt', 'dwindl', 'chosen', 'dispair', 'encourag', 'skrew', 'physical', 'trial', 'valuabl', 'consult', 'dat', 'm.p.h', 'spoke', 'aband', 'coapl', 'mistake.', '…simpli', 'therefour', 'bulid', 'cimat', 'builid', 'peol', 'direcion', 'driver', 'singl', 'gruel', 'recip', 'disast', 'debilit', 'sunlight', 'proper', 'gean', 'toler', 'individu', 'importantli', 'within', 'evapor', 'aren', 'nowaday', 'eccentr', 'foolish', 'advice…', 'drunk', 'whatev', 'persuad', 'warmth', 'delay', '+', 'hasn', 'rang', 'cruelti', 'spetator', 'war-last', 'uncap', 'lest', 'braid', 'collect', 'old-', 'centr', 'ststed', 'inward', 'se', 'uple', 'sheed', 'figur', 'ware', 'reservoir', 'disappear', 'canal', 'touch', 'attempt', 'enlight', 'succul', 'ici', 'affet', 'afew', 'ariv', 'drivu', 'hegot', 'werent', 'defenitli', 'difficut', 'launch', 'nd', 'toslic', 'park.', 'regularlyand', 'yoth', 'suggest', 'starter', 'alter', 'himself.th', 'fierc', 'prowl', 'concerv', 'anybodi', 'nearbi', 'shelter', 'predomin', 'requir', 'threaten', 'grade', 'gaze', 'breath', 'catch', 'posabl', 'lesi', 'space', 'portron', 'sent', 'rote', 'risum', 'weter', 'nex', 'pin', 'distinct', 'partial', '/', 'gradual', 'amus', 'agit', 'dent', 'beyond', 'repair', 'breaksdown', 'reflect', 'carefre', 'them.', 'resort', 'ammount', 'shot', 'retard', 'reall', 'baren', 'exsusa', 'ater', 'enemi', 'chrest', 'insight', 'dirk', 'sold', 'gett', 'boulder', 'curri', 'sweet', 'lough.', 'circumst', 'treachor', 'stretch', 'hardli', 'soccer', 'game', 'ball', 'tack', 'dehydrati', 'sore', 'background', 'theme', 'indespair', 'atleast', 'anyway', 'blackish', 'rigor', 'effectliy', 'imaginari', 'fictiou', 'stream', 'cave', 'persev', 'ceas', 'till', 'cleari', 'waveng', 'gears.', 'eat', 'hil', 'gohst', 'uphil', 'flatroad', 'crest', 'preti', 'friend', 'fechmisburg', 'pose', 'obviou', 'renew', 'list', 'onto', 'crawl', 'neat', 'scaok', 'â€\\x9d', 'limit.â€\\x9d', 'suppl', 'wisli', 'date1.', 'doom', 'afect', 'probali', 'call', 'stope', 'gart', 'entitl', 'tagon', 'manner', 'bodili', 'unpav', 'pave', 'exaust', 'reliv', 'outlook', 'comparison', 'reviv', 'comic', 'neav', 'neov', 'greater', 'alo', 'serv', 'nowev', 'stage', 'anang', 'confidi', 'desser', 'ago', 'fist', 'realist', 'version', 'diplet', 'sank', 'destroy', 'rate', 'bitten', 'cyclists.th', 'foreign', 'mistake.aft', 'people.even', 'troublesom', 'too.although', 'through.fight', 'honor', 'back-', 'obviosli', 'durastr', 'produced.', 'caps1.', 'turain', 'souldn', 'vigor', 'exercis', 'neccesari', 'detour', 'normal', 'gorgeous', 'vocubulari', 'conclud', 'stock', 'detirmin', 'shortli', 'graciou', 'negit', 'throw', 'track', 'num1°', 'caps1.a', 'bearli', 'num2°', 'caps3', 'comblain', 'view.', 'beth', 'decreas', 'o', 'f', 'building-abandon', 'ealier.', 'imposs', 'tarren', 'midwest', 'quest', 'note', 'ear', 'aspect', 'frol', 'inord', 'chair', 'it.', 'kermaski', 'usal', 'temp', 'weath', 'souround', 'blister', 'closer', 'smoggi', 'closet', 'vividli', 'conscion', 'flustrat', 'met', 'op', 'endlessli', 'bleak', 'spoil', 'iin', 'faucet', 'streanth', 'supply.', 'feet', 'supplyl', 'incorrect', 'energy.thes', 'feacher', 'drinkabl', 'spring', 'popul', 'industri', 'abadond', 'apon', 'dur', 'frequent', 'dow', 'pritti', 'dezert', 'califonia', 'constantli', 'your-self', 'location2', 'newli', 'soo', 'thon', 'compar', 'climber', 'storm', 'location3', 'strone', 'hypothermia', 'mail', 'box', 'unconsci', 'electrolit', 'fort', 'pessamist', 'interf', 'group', 'trow', 'adjust', 'somebodi', 'narrit', 'pourch', 'trie', 'gbost', 'reasur', 'cautrou', 'whelch', 'abundand', 'inter', 'awil', 'half', 'rember', 'abait', 'swallow', 'finali', 'insid', 'grate', 'goet', 'rusty-bumpi', 'dehygr', 'plane', 'portray', 'respons', 'threat', 'okay', 'panick', 'afterward', 'rattlesnak', 'mere', 'statement.', '–say', 'chuck', 'water.', 'activ', 'feelslik', 'perish', 'nupe', 'emit', 'sewag', 'incred', 'distress', 'estim', 'maijor', 'unexpect', 'distant', 'hear', 'forev', 'horrifi', 'cant', 'blaze', 'elimin', 'challeg', 'sile', 'lostli', 'dramaticali', 'callada', 'woudn', 'hte', 'no-wher', 'refudg', 'panic', 'jog', 'extend', 'unmark', 'irrisist', 'charg', 'unsteadi', 'yosemitr', 'raha', 'lay', 'grassi', 'floweri', 'chalang', 'workout', 'dismay', 'flatland', 'morph', 'await', 'suddenli', 'amaz', 'feautur', 'humbl', 'despar', 'hopful', 'cous', 'strugl', 'abandond', 'oldtim', 'afaid', 'dihydr', 'job', 'worng', 'nautiou', 'stort', 'expessari', 'no-', 'sight.', 'path.', 'diamondback-', 'majoritli', 'terran', 'hgh', 'deserts-al', 'mle', 'thr', 'affecton', 'preclaminatli', 'nit', 'demonsrtr', 'wate', 'owner', 'neighbar', 'inf', 'nthi', 'agarsear', 'lane', 'infest', 'lars', 'madon', 'liter', 'acn', 'countri', 'indifferrenth', 'deterrior', 'uncer', 'chunk', 'novel', 'champion', 'art', 'exhibit', 'wave', 'hug', 'star', 'stip', 'rash', 'night', 'un', 'himilet', 'pay', 'attent', 'injur', 'poisnou', 'cycclist', 'bite', '.there', 'change.', 'feelso', 'intitl', 'reserv', 'woke', 'copl', 'old-timers.', 'shile', 'baitshop', 'blue', 'in…mi', 'brain.', 'spent', 'building/town', 'v.', '…haven', 'office.', 'rundown', 'occur', 'ultim', 'brought', 'nort', 'motorcycl', 'vacat', 'strat', 'eidanc', 'prime', 'heartstrok', 'gentelmen', 'welm', 'urs', 'ox', 'base', 'shirt.', 'anyhil', 'supply.th', 'leak', 'landmark', 'statu', 'butteri', 'azyl', 'heatatack', 'herat', 'bumpsi', 'veget', 'disprat', 'stand', 'smart', 'caps1.th', 'stronger', 'eventhough', 'battl', 'opposit', 'unfortonatli', 'marathon', 'request', 'amajor', 'non-pav', 'meneuv', 'mesem', 'behav', 'improv', 'simpl', 'enabl', 'ramshack', 'unplan', 'among', 'judgment', 'asced', 'sp', 'cing', 'wear…and', 'tear', 'chuckled…', 'unsettl', 'optomist', 'expans', 'depress', 'abandur', 'undrink', 'perserver', 'ridicuosli', 'correal', 'regulari', 'pedai', 'life-or-death', 'concious', 'ex', 'supos', 'comu', 'travll', 'posses', 'retain', 'heard', 'symptom', 'tempertur', 'tiredd', 'fataus', 'becau', 'time1', 'traditon', 'drove', 'gentli', 'metaphor', 'firt', 'watwer', 'dyhiger', 'funcat', 'mud', 'gergeou', 'extremli', 'detinit', 'snott', 'advis', 'mabey', 'unbeleiv', 'redicul', 'overh', 'dyhdrat', '…look', 'pavement…', 'spite', 'gaven', 'pedd', 'everywher', 'safti', 'stoff', 'trick', 'earlier.', 'realiv', 'profound', 'solitud', 'seclut', 'hard-work', 'upsemit', 'ramshacklesh', 'alittl', 'curiou', 'focus', 'manymor', 'num4', 'num3.thi', 'bust', 'vast', 'assist', 'effective.', 'snake-it', 'squirrel', 'clash', 'wash', 'sartisfact', 'red', 'volcano', 'milker', 'swampland', 'california.', 'affirm', 'regularly.', 'ganged-up', 'unawar', 'ignor', '…thi', 'time…but', 'bought', 'remin', 'movi', 'awhil', 'ove', 'glad', 'factory-', 'time-', 'intensifi', 'reveal', 'mirror', 'exclaim', 'it-', 'factory.', 'olw', 'uninhabit', 'deciv', 'mistaken', 'irrit', 'debri', 'caneffect', 'inculd', 'suriond', 'commun', 'sea', 'surf', 'son', 'body-surf', 'occup', 'secod', 'diffent', 'brink', 'exposur', 'powerless', 'towns-non', 'promising-', 'arundown', 'satisfi', 'seel', 'everwel', 'attud', 'dispot', 'urgenc', 'similair', 'backpack', 'bear', 'essenti', 'enough.', 'peldder', 'mouth.', 'reduc', 'crippil', 'mo', 'acid.then', 'immedi', 'length', '.on', 'bicicl', 'pramb', 'his/her', 'realiti', 'colifornia', 'six', 'seven', 'definet', 'cycalist', 'navig', 'stamina', 'desteart', 'la', 'hang', 'miseri', 'afet', 'alot.th', 'leaf', 'b/c', 'wi', 'happier', 'rigur', 'higher', 'sky', 'pall', 'froz', 'maling', 'lol', 'tabe', 'timid', 'aga', 'dimond', 'caps4', 'example.', 'hill…', 'athlet', 'arn', 'tempatur', 'battarey', 'ill', 'fli', 'bar', 'eaten', 'energy-al', 'inconveni', 'becas', 'scari', '…and', '…ride', 'fine.', 'inusu', 'action', 'deeper', 'scarciti', 'dictat', 'tought', 'waiter', 'dink', 'massiv', 'snake…', 'thristi', 'held', 'prevent', 'necess', 'trigger', 'scream', 'annoy', 'instinct', 'remov', 'throuout', 'releiz', 'wisdom', 'porches.flat', 'nag', 'sensat', 'faulti', 'inadequet', 'mid-', 'trap', 'consist', 'dirachion', 'haed', 'nam', 'lesort', 'inhabit', 'hummid', 'collet', 'barley', 'c', 'regreat', 'dident', 'affectd', 'frustar', 'expeci', 'laffect', 'transport', 'fellow', 'struck', 'thrist', 'tip', 'yo', 'seal', 'lent', 'henc', 'shoot', 'stamena', 'trudg', 'camp…', 'hon', 'tantallizg', 'snak', 'back–blok', 'pavement.', 'bin', 'ruff', 'sandi', 'cass', 'outhor', 'hasno', 'exasper', 'author/cyclist', 'advertis', 'crueler', 'sadist', 'dehider', 'dileri', 'vision', 'distort', 'shadi', 'horrid', 'winter', 'expert', 'identifi', 'provis', 'self-', 'destination.for', 'sharp', 'self–determin', 'kick', 'energis', 'anything.thi', 'summertim', 'applaud', 'diary.', 'distarr', 'guess', 'sofcti', 'yay', 'swore', 'siga', 'comp', 'savior', '…work', 'out….', 'wo', 'freeli', 'guccher.and', 'ocean', 'board', 'whinedi', 'wheather', 'scorc', 'snake…block', 'agasint', 'nature.', 'is.', 'descipt', 'tantilis', 'draw', 'disgust', '.for', 'flovor', 'settl', 'misreabl', 'elgo', 'quickest', 'real-lif', 'hindranc', 'stood', 'bloze', 'room', 'scorn', 'depict', 'step', 'simplist', 'lifeless', 'waterless', 'manev', 'tempt', 'parch', 'beacon', 'third', 'peppl', 'field', 'raod', 'crushal', 'cationli', 'tempitur', 'dwind', 'hope-al', 'perspir', 'appoint', 'skin', 'function', 'neth', 'probli', 'falt', 'school', 'polut', 'unimport', 'non-upd', 'acual', 'littlest', 'ld', 'nightmar', 'vow', 'peck', 'slice', 'perfus', 'ways.for', 'june.th', 'passout', 'die.ther', 'deserts.th', 'perspor', 'flood', 'analyz', 'disapoint', 'sappli', 'assault', 'instruct', 'num1/=', 'tempet', 'tender', 'frist', 'aditud', '.how', 'diffuculti', 'charcter', 'emphasis', 'dilut', 'frighten', 'neurot', 'straw', 'outrag', 'spare', 'dread', 'desolut', 'mimic', 'dosen', 'rais', 'delight', 'outcast', 'limb', 'gust', 'cow', 'relic', 'alamond', 'val', 'dangerai', 'thinkof', 'wirt', 'nite', 'drie', 'soil', 'pert', 'ya', 'dig', 'ia', 'appeal', 'sarcast', 'american', 'onnli', 'menatali', 'sabstat', 'tird', 'baidi', 'setback', 'concern', 'shini', 'slope', 'rein', 'couragi', 'positivli', 'band', 'dangou', 'dicrid', 'discrid', 'changing.', 'dific', 'journi', 'sariv', 'accur', '/she', 'chest', 'boundari', 'unwar', 'termin', 'fourti', 'dissetl', 'ramshacl', 'strech', 'brang', 'discaurag', 'mock', 'ledg', 'throat.', 'solv', '.the', 'altogeth', 'hills-al', 'redicui', 'aqust', 'muski', 'imageri', 'shoe', 'varia', 'moobl', 'hopele', 'anywer', 'shoutcut', 'camp.thi', 'sicocss', 'conlusio', 'traini', 'swim', 'everyday', 'doubtful', 'biva', 'entri', 'prais', 'direction.', 'broken', 'shorter', 'sticki', 'center', 'brilliant', 'manuev', 'concerned/scar', 'cyclit', 'displeasur', 'exagger', 'deger', 'sop', 'complic', 'nowhea', 'source.', 'sink.', 'regan', 'tedious-al', 'isnt', 'tediou', 'acced', 'gether', 'traver', 'cosin', 'theer', 'poich', 'calter', 'youg', 'maid', 'backin', 'bycycl', 'disaster', 'infect', 'envior', 'tye', 'poisen', 'desis', 'carlifonian', 'ride.ghost', 'yosenite.th', 'syndiz', 'desrt', 'dowdn.th', 'barrier', 'rattlesn', 'horror', 'hell', 'serpent', 'dehydration.al', 'yosenit', 'wet', 'slip', 'helmet', 'unfocu', 'worm', 'essay/', 'quaint', 'bustl', 'decim', 'worse-', 'perspect', 'fool', 'discomfort', 'supposedli', 'pleasur', 'semi', 'brutal', 'oo', 'tumberwe', 'back-beak', 'confict', 'extent', 'beed', 'ton-lik', 'out.', 'depleat', 'term', 'treck', 'dehedr', 'persu', 'precari', 'provid', 'water.thi', 'question', 'answer', 'bent', 'constintli', 'thrister', 'riducul', 'ruggid', 'confront', 'cogger', 'smile.', 'contact', 'cue', 'incount', 'ramshacki', 'wick', 'victim', 'hinder', 'gost', 'appropri', 'impar', 'maintain', 'wether', 'calv', 'steadili', 'ore', 'lump', 'lift', 'outdat', 'excel', 'bumpier', 'plain', 'crack', 'gallon', 'pasag', 'botha', 'civilles', 'paridic', 'phrase', 'steadi', 'peddel', 'undergo', 'nothing', 'empress', 'whe', 'drowsi', 'lnad', 'gaid', 'perserv', 'perserverd', 'blackout', 'item', 'violent', 'soud', 'forsaken', 'tap', 'shine', 'referr', 'math', 'willing', 'posot', 'thro', 'limit.th', 'were.it', 'te', 'times.they', 'nearest', 'town.th', 'gut', 'sunset', 'kid', 'georgu', 'listend', 'gentlemen.', 'total', 'absenc', 'exhausgen', 'content', 'lover', 'confidentin', 'theirjudg', 'deserted.though', 'laster', 'toxic', 'ofa', 'guzzl', 'overpow', 'hap', 'repeat', 'win', 'dder', 'intellig', 'extrat', 'sukker', 'hike', 'dieng', 'sworn', 'self-control', 'paid', 'circulate.', 'hallucin', 'thine', 'out.th', 'june.a', 'wrong.in', 'state.th', 'setting.everyth', 'problems.th', 'woud', 'probablli', 'organization2', 'greatlli', 'expirenc', 'deseret', 'temptur', 'lit', 'agil', 'climax', 'certin', 'desrib', 'difinit', 'scketl', 'aroun', 'sorch', 'soluat', 'roads.thi', 'of.ther', 'happen.y', 'screw', 'gravel', 'atthat', 'bud', 'judjment', 'exoust', 'exsist', 'dehidart', 'fourther', 'barin', 'eth', 'clearlier', 'intend', 'june….', 'shove', 'severli', 'facil', 'intimid', 'bound', 'eseanc', 'hab', 'bag', 'thereaft', 'amiss', 'num3.', 'strike', 'origin', 'entil', 'abid', 'reptil', 'tenaci', 'eighther', 'excruci', 'cheer', 'conscious', 'seemingli', 'uniqu', 'exsost', 'out-dat', 'tent', 'challang', 'heatstroak', 'prone', 'ocha', 'totali', 'mid', 'significantli', 'boil', 'consumpt', 'salt', 'wound', 'daunt', '–all', 'trave', 'ling', 'throug', 'late', 'short-cut', 'flip', 'californa', 'sins', 'artifact', 'museum', 'no-on', 'shutdown', 'drama', 'multipli', 'hint', 'ife', 'slep', 'arrest', 'indic', 'challamg', 'eng', 'brief', 'preview', 'involv', 'noon', 'rope', 'cord', 'slight', 'strictli', 'obey', 'discov', 'immens', 'theyr', 'sing', 'watter', 'velch', 'unuseland', 'oppos', 'mirge.', 'atmospher', 'gosh', 'judg', 'unsaf', 'disgard', 'neighbor', 'ray', 'catylist', 'cool.', 'risl', 'satiricl', 'stumbel', 'pitur', 'num1.', 'previou', 'num2.', 'atm.', 'exosti', 'wilder', 'undevelop', 'recourc', 'soliter', 'safer', 'restless', 'rumshackl', 'benefici', 'uplift', 'portrin', 'water-delet', 'beginin', 'four', 'thirteen', 'swear', 'upheat', 'raugher', 'civilis', 'shor', 'flatter', 'relylitt', 'lon', 'dull', 'desolet', 'stricken', 'hot.in', 'dehydrad', 'tempur', 'dog', 'bootl', 'pattern', 'swreat', 'drier', 'shake.', 'rond', 'thirti', 'correctli', 'down…', 'rune', 'muck', 'adher', 'confins', 'sin', 'sorr', 'kness', 'rast', 'totalt', 'bum', 'roadblock', 'desperatli', 'poseh', 'nuerteci', 'fulli', 'conscienc', 'journal', 'posibl', 'cri', 'diploma', 'cali', 'quet', 'civilaz', 'thre', 'ridin', 'deyhdrat', 'pass-out', 'substain', 'ofter', 'hisself', 'consev', 'thath', 'jouney', 'elabor', 'whetther', 'glimps', 'mountian', 'region', 'wither', 'expearenc', 'talik', 'abd', \"'ll\", 'sa', 'throuhg', 'eimit', 'endag', 'somewhere.', 'mself', 'treat', 'persit', 'motor', 'countrysid', 'hail', 'getin', 'accid', 'hair', 'syo', 're-hydr', 'drecionsn', 'resign', 'slightli', 'resembl', 'affor', 'nowhere-', 'camel', 'wearili', 'zone', 'prolong', 'nessiarri', 'successs', 'difificulti', 'unbeliev', 'through-out', 'satisfact', 'info', 'haden', 'tool', 'potenti', 'honey', 'comb', 'waterbottl', 'acsess', 'firm', 'rall', 'easli', 'tricki', 'sharpli', 'enerf', 'firstli', 'crucial', 'wing', 'strok', '…water', 'degrees…when', 'thrist…', 'horizon/hil', 'alright', 'olcasli', 'smoth', 'amplifi', 'cost', 'mesag', 'blindli', 'releax', 'carrol', 'gate', 'uncocl', 'dehirr', 'moment', 'manyway', 'thiristi', 'shacl', 'sevral', 'lamestm', 'egreet', 'factt', 'onemor', 'pedalind', 'agreat', 'oneth', 'homthes', 'incas', 'meloncholi', 'load', 'futher', 'freshen', 'persav', 'reek', 'abbandon', 'uncar', 'thire', 'gentl', 'highli', 'confied', 'andor', 'extreamli', 'steyetl', 'yesterday', 'younger', 'disori', 'mirrag', 'photo', 'reaction', 'behavior', 'rafh', 'rethink', 'judgement', 'showshow', 'majori', 'colud', 'grope', 'lousi', 'practic', 'onelong', 'unwant', 'enogth', 'doubl', 'wip', 'toughest', 'neighbourhood', 'shake', 'sevior', 'terria', 'dins', 'faintid', 'peatl', 'indecrias', 'presentid', 'fetal', 'exponeutali', 'prodiom', 'ther', 'creul', 'batey', 'tay', 'pluse', 'snakes.it', 'rideto', 'encout', 'oldmen', 'lament', 'lecatraci', 'encounterd', 'epidem', 'cyxlist', 'daili', 'routin', 'storygreatli', 'pitti', 'roughli', 'strest', 'horabl', 'abut', 'terair', 'shard', 'winday', 'said.h', 'waterdeplet', 'fuier', \"doesn'twnt\", 'reguarli', '…the', 'hilusin', 'setttl', 'highdrat', 'smoot', 'thirst.', 'treain', 'condis', 'prodlem', 'prasor', 'strock', 'stll', 'factri', 'seclus', 'sike', 'arctic', 'dissappoint', 'energet', '.after', '.first', 'boom', 'prosper', '.they', '.flat', 'deliri', 'dissapoint', 'partli', 'drawn', 'happens.if', 'hopelssli', 'lif', 'ok.', 'ispir', 'stey', 'optionist', 'fluent', 'barst', 'rectev', 'fallen', 'biten', 'daleri', 'retir', 'rejuven', 'mid-day', 'boldli', 'coincid', 'benefit', 'cocnlus', 'sheet', 'unus', 'complex', 'rub', 'ined', 'excitedli', 'particulali', 'oppon', 'particularlli', 'anytim', 'hillier', 'peadl', 'picnic', 'increasingli', 'mardid', 'hip', 't/couldn', 'whelm', 'alatin', 'dffinat', 'dvun', 'terrien', 'lisin', 'fowk', 'hav', 'apirean', 'howl', 'fella', 'wouldint', 'whatsoev', 'ozz', 'charat', 'sarley', 'versu', 'sap', 'summari', 'parentasi', 'father', 'ca', 'strerigth', 'king', 'way.it', 'narror', 'while.th', 'fock', 'shed.', 'confirm', 'here.ani', 'pessimism.eventu', 'declar', 'idea.thi', 'hate', 'fault', 'moreov', 'dehydryeon', 'aern', 'battery-oil', 'blew', 'fauret', 'fun', \"does'nt\", 'bein', 'banser', 'posin', 'scan', 'sot', 'dischric', 'dime', 'babi', 'nuvv', 'carid', 'coudnt', 'pourng', 'desort', 'tongu', 'she/h', 'til', 'more.', 'white', 'noun', 'allof', 'whereto', 'pump.', 'taint', 'manevu', 'reload', 'replerilsh', 'werri', 'retent', 'fem', 'stare', '–hundr', 'settin', 'instenc', 'wid', 'sweeti', 'for.', 'unconfid', 'becus', 'biter', 'attiud', 'ister', 'exsaus', 'jaunt', 'dired', 'occas', 'averag', 'abondon', 'singn', 'reacher', 'bolt', '$', 'surroud', 'indiffer', 'redavean', 'petti', 'ley', 'roads.also', 'riai', 'nae', 'tase', 'on.also', 'keypoint', 'waa', 'mentli', 'phyisicli', 'evergreen', 'mute', 'will-pow', 'strenght', 'heatstor', 'stoni', 'buid', 'peadil', 'pargraph', 'ment', 'directin', 'poarch', 'advantag', 'rosemit', 'rythm', 'triumph', 'psycolig', '....', 'sane', 'caues', 'malic', 'drunken', 'fountan', 'miles.', 'obstical', 'travilng', 'triy', 'altitud', 'mother', 'toiledon', 'louag', 'jug', 'proud', 'display', 'erod', 'compet', 'thin', 'hoppi', 'bcous', 'heathi', 'becass', 'soid', 'bond', 'exguast', '.do', '.these', '.becaus', '.not', 'cyclistsoon', 'unoccupi', 'poem', 'culous', 'land.', 'exchang', 'exauht', 'salvat', 'conclut', 'built', 'desir', 'drench', 'upbeat', 'comesto', 'presum', 'gulp', 'restroom', 'spectaculiar', 'chuckel', 'roud', 'mat', 'fairli', 'fate', 'deject', 'admit', 'defeat', 'mix', 'grant', 'willpow', 'larnest', 'psycholog', 'pleas', 'resolv', 'truth', 'turmoil', 'oneself', 'persimist', 'sought', 'meal', 'multipl', 'salidris', 'adaodo', 'frot', 'strote', 'adaodon', 'dehydratiog', 'exausit', 'listn', 'suckenc-ston', 'exhust', 'rel', 'nia', 'protect', 'occupi', 'frustat', 'palc', 'gey', 'oot', 'envicu', 'enviou', 'al', 'water-depl', 'partner', 'share', 'rheir', 'placewa', 'chew', 'turan', 'glimmer', 'achiv', 'poy', 'desent', 'thot', 'yuuth', 'folluw', '–an', 'foctari', 'camp-if', 'pur', 'tostick', 'purchess', 'hce', 'h', 'curb', 'whip', 'cicl', 'awe', 'tha', 'tthere', 'tastd', 'max', 'one-fifth', 'polant', 'clinch', 'surg', 'cockish', 'somelan', 'lijid', 'ohang', 'determain', 'discrib', 'drippen', 'slump', 'wall', 'temperatar', 'clyclist', 'roadway', 'foot', 'decades.', 'hassel', 'crook', 'war-', 'candi', 'strangers.', 'shortcut.when', 'fine.but', 'building.bi', 'water.ther', 'drink.h', 'thirsty.th', 'easier.about', 'harder.toward', 'factory.thi', 'juice.in', 'pait', 'ramsackl', 'anther', 'actair', 'expeciali', 'discript', 'overtim', 'future.', 'lie', 'irrespons', 'enough…', 'roads…', 'map.', 'exsempl', 'git', 'dehyjradid', 'everychang', 'refrenc', 'woman', 'road.', 'associ', 'instil', 'remark', 'via', 'albeit', 'jokingli', 'stack', 'lase', 'saftey', 'dye', 'outand', 'batteryacid', 'elat', 'kurmashi', 'heavili', 'shorecay', 'separ', 'lard', 'exspeci', 'acide.', 'treater', 'deseart', 'visit', 'feater', 'deloshion', 'dehidrashion', 'balinc', 'dowen', 'signifig', 'excut', 'coolest', 'seraneti', 'shock', 'trowol', 'integr', 'tualli', 'scortch', 'poisio', 'terrifi', 'dismiss', 'minor', 'pow', 'endoor', 'flatten', 'oasi', 'beat-up', 'domper', 'enhanc', 'wouldv', 'towns-ghost', 'scald', 'goten', 'conditon', 'pointless', 'lotof', 'smog', 'june.their', 'nativ', 'teach', 'trough', 'litherali', 'contin', 'dous', 'insuffici', 'gratitud', 'pre', 'pare', 'irrat', 'leval', 'liesurli', 'hyperbol', 'exager', 'paint', 'literay', 'biclist', 'pretens', 'wisdom.', 'astonish', 'appreci', 'wn', 'emerg', 'cell', 'phone', 'wass', 'past.', 'fish-if', 'tolld', 'context', 'drug', 'tot', 'lessen', 'earlier-had', 'despers', 'vagu', 'info-al', 'soae', 'exauston', 'thea', 'stupid', 'unhealthi', 'profession', 'dress', 'sudden', 'inhanc', 'cyclisst', 'catu', 'suff', 'surfer', 'moreaccur', 'million', 'rameshackl', 'ma', 'lizard', 'posion', 'organ', 'spiket', 'gross', 'wit', 'hungri', 'insur', 'sall', 'porcher', 'old-folk', 'saccur', 'lame', 'in-', 'anyl', 'well-need', 'droop', 'angst', 'highlight', '…but', 'thirst…', 'filthi', 'ideal', 'glitch', 'fatur', 'imagton', 'short-cut.', 'en', 'jag', 'iso', 'tamest', 'influent', 'exauhst', 'cycllist', 'bege', 'exlain', 'californian', 'rollar', 'coaster', 'bless', 'puffi', 'cloud', 'cyclisti', 'sructur', 'clan', 'bacam', 'sunshin', 'anymo', 'confidenc', 'fret', 'heartt', 'logic', 'bobi', 'normali', 'sneak', 'expedit', 'bland', 'calmer', 'sycolog', 'devic', 'worried/', 'dehightr', 'cyclcist', 'finnali', 'grandpar', 'froit', 'n.c', 'wheel', 'sffect', 'infrastructur', 'colun', 'streetsign', 'outing', 'observ', 'physicali', 'savor', 'ridicu', 'narrar', 'legibl', 'ruun', 'harden', 'tamblewe', 'tp', 'dsown', 'substitut', 'instinct.', \"'re\", 'born', 'territori', 'excersis', 'adapt', 'jack', 'fire', 'pit', 'vukor', 'ovetook', 'canstrant', 'ird', 'oper', 'power/effici', 'depriv', 'stroke.', 'setl', 'youself', 'consicencess', 'concis', 'trechor', 'rippl', 'strra', 'provok', 'relis', 'feeling.', 'dirty', 'strength.', 'sacrific', 'hish', 'dodg', 'thrivin', 'draper', 'herb', 'expier', 'wride', 'gunk', 'presid', 'welcom', 'hi/her', 'desend', 'convinc', 'bycl', 'con', 'camper', 'outof', 'curv', 'sarv', 'thisti', 'grude', 'uneasi', 'inabl', 'tollon', 'aplist', 'alarm', 'exospod', 'spell', 'sped', 'intrust', 'disown', 'cycalrst', 'shave', '…tell', 'method', '…slowli', 'seccondli', 'accross', 'tyre', 'illusionalist', 'abound', 'mountin', 'oledrar', 'deci', 'injuri', 'short-stori', 'os', 'life-thr', 'excedingli', 'gun', 'romshackl', 'muic', 'replesh', 'unpleas', 'leas', 'torment', 'codi', 'supli', 'fealt', 'thankful', 'towand', 'yellow', 'beastli', 'faith', 'lesion', 'pust', 'affec', 'lope', 'hoy', 'hotn', 'on.', 'muster', 'soak', 'bairli', 'wobbl', 'fetur', 'enegi', 'whant', 'drag', 'helps…with', 'verbal', 'deliv', 'alongrid', 'fromth', 'neer', 'story.th', 'drink.th', 'goreg', 'broke', 'barron', 'hapi', 'sooooooooooooo', 'happsi', 'woohoo', 'heartstok', 'unaffect', 'suitabl', 'dampen', 'ragh', 'goast', 'unliv', 'specif', 'crppling', 'illus', 'row', 'overli', 'sixti', 'out-of-d', 'equip', 'hotel', 'von', 'sourround', 'format', 'day….', 'sert', 'cane', 'mercilessli', 'senteanc', 'comment', 'obsticul', 'nesl', 'yellc', 'suppor', 'trecher', 'surpass', 'prevail', 'purpos', 'sectiv', 'audienc', 'say/think', 'butt', 'shep', 'diform', 'evey', 'diferen', 'heartsrok', 'pedalist', 'eneegi', 'deserat', 'co', 'smoki', 'modifi', 'percept', 'mach', 'tice', 'himout', 'tumpl', 'oll', 'tide', 'beceus', 'mough', 'useable.', 'flower', 'quickly.', 'afftect', 'pop', 'follw', 'od', 'bandon', 'beatdown', 'furiou', 'delici', 'anxiou', 'skeptic', 'mens-direct', 'strip', 'inforc', 'punish', 'wuold', 'rage', 'shuold', 'peen', 'succes', 'difficul', 'error', 'peek', 'wig', 'dyhrat', 'sreach', 'steal', 'riht', 'gian', 'withen', 'obstic', 'a.k.a', 'lession', 'respond', 'yes-', 'understood', 'neihbor', 'hood', 'hoter', 'tranquil', 'pleasant', 'souther', 'resolut', 'reserveir', 'popular', 'quinch', 'solut', 'meschi', 'scorg', 'moth', 'thirster', 'maker', 'colorado', 'killer', 'cors', 'bevey', 'likli', 'lighter', 'irriat', 'hm', 'him.besid', 'heighten', 'man-mad', 'subconsci', 'growth', 'invalid', 'pang', 'precaut', 'desert-', 'recal', 'beati', 'restor', 'mlle', 'changin', 'modem', 'spar', 'lush', 'beli', 'overhead', 'trek', 'ploven', 'vestig', 'scenario', 'thribiy', 'cyclisit', 'yosemite.when', 'survivor', 'speeed', 'side-ov', 'repeatedli', 'kne', 'fele', 'ooey', 'lonely', 'qoutes.', 'harsher', 'worst.th', 'experianc', 'graund', 'ton', 'hop', 'loze', 'extens', 'dehydration.th', '…follow', 'pedalig', 'thickest', 'fifty-f', 'luckli', 'dilema', 'ecas', 'dalv', 'unforgiv', 'plannet', 'host', 'tour', 'luxuri', 'coast', 'thive', 'dep', 'statr', 'buzz', 'gladli', 'asur', 'puform', 'motu', 'matt', 'includesouli', 'reform', 'advanc', 'pufornen', 'hill-and', 'layout', 'dude', 'so-', 'snake….', 'basketbal', 'tantil', 'bike-rid', 'stagger', 'caldn', 'recycl', 'trash', 'soda', 'geoger', 'seeneri', 'critic', 'nervi', 'enduranc', 'houever', 'smili', 'signal', 'tine', 'prit', 'romd', 'tormil', 'heven', 'wher', 'shift', 'dilema.', 'the…', 'freak', 'diamondback.', 'walru', 'grab', 'pebbles.', '*', 'stuctur', 'worser', 'journey.th', \"'not\", 'goig', 'timerd', 'stab', 'selfish', 'tho', 'taught', 'lil', 'toke', 'caps2-al', 'ned', 'terrein', 'porblem', 'forexampl', 'nottic', 'erect', 'totzi', 'himslef', 'setent', 'confiendc', 'syclist', 'codgers.', 'foucet', 'profus', 'otherwis', 'unavail', 'traumat', 'thirsty-', 'redufrom', 'ram-shackl', 'pumps.', 'mph.', 'crop', 'topic', 'inspect', 'resent', 'alerg', 'peanut', 'slept', 'lunch', 'starv', 'restaur', 'sinle.', 'hunger', 'sush', 'favorit', 'shore', 'ineffici', 'horid', 'optim', 'g', 'lareg', 'fare', 'delusion', 'month.ther', 'particularli', 'even-hotter-than-norm', 'grassland', 'dreck', 'wasteland', 'buzzard', 'cowskul', 'intimidi', 'reput', 'now-teriffi', 'farmland', 'neither', 'travers', 'ugli', 'raini', 'match', 'verri', 'ff', 'ong', 'berough', 'apropreit', 'lyand', 'tiresa', 'inflat', 'properli', 'stopat', 'tumber', 'back-back', 'footbal', 'upand', 'spot…', 'eary-summ', 'proce', 'rutsi', 'surfag', 'heatrok', 'bike-ridden', 'prece', 'land-mark', 'rip', 'exciter', 'shred', 'dried-out', 'negitvli', 'negitv', 'settnig', 'alaska', 'hypo', 'thermia', 'ghoust', 'determind', 'eb', 'dought', 'polit', 'vital', 'sluggish', 'discour', 'trake', 'valley', 'trakl', 'lotat', 'discurag', 'clame', 'loot', 'cme', 'educ', 'friendship', 'doth', 'exhaugnt', 'detect', 'confess', 'fisherman', 'wornout', 'kurmaki', 'easiest', 'ragadi', 'naraer', 'moistur', 'door', 'pars', 'obnon', 'hassh', 'interept', 'thisi', 'consent', 'understandmet', 'refus', 'hardi', 'saniti', 'recollect', 'dose', 'plege', 'watrer', 'rach', 'resist', 'conscies', 'capac', 'wath', 'reflector', 'herend', 'onpil', 'num5', 'num7', 'hing', 'desertd', 'distrat', 'contour', 'possibley', 'lersuri', 'surrond', 'appert', 'rought', 'afit', 'bluee', 'sumertim', 'around.', 'ra', 'dout', 'self-pitti', 'name.', 'allreadi', 'acus', 'guid', 'inhabin', 'nouroush', 'caation', 'ching', 'wherea', 'immensli', 'mover', 'changed.that', 'seed', 'farm.', 'point.', 'run–down', 'instal', 'hid', 'lonli', 'persist', 'kiil', 'unreli', 'deston', 'net', 'factal', 'shold', 'ahad', 'dimeril', 'sandblust', 'generili', 'quest-lik', 'ghost-town', 'loneli', 'somewahat', 'grim', 'sour', 'sadli', 'hopelessli', 'technolog', 'crulest', 'hardiest', 'respect', 'overheat', 'foul', 'certainli', 'fait', 'landscape.th', 'fone', 'negg', 'tens', 'sedr', 'mear', 'distin', 'cycist', 'intern', 'tramat', 'inch', 'abanodon', 'finnal', 'adon', 'dierect', 'tourtement', 'discoverag', 'ramshacur', 'youknow', 'cyclistand', 'narr', 'stale', 'exactig', 'swelt', 'merciless', 'eyeish', 'incomplet', 'unyield', 'count', 'throgh', 'shade-al', 'pond', 'actuali', 'begar', 'settlement', 'sail', 'directionsand', 'erganc', 'subburb', 'grave', 'n', 'crucel', 'fail', 'undesir', 'special', 'ramshackli', 'hamest', 'me.thi', 'discharg', 'unbareabaln', 'endless.then', 'thrust', 'water-depleti', 'sanctur', 'constrain', 'regrat', 'sorrow', 'ripe', 'postal', 'hourli', 'distrought', 'sadists…', 'updat', 'cloe', 'anxieti', 'minitur', 'environment', 'subject', 'intef', 'usuali', 'nearer', 'blazen', 'scroach', 'panickli', 'skirt', 'exhast', 'stirv', 'dehidad', 'imagen', 'hime', 'yosomit', 'coheratli', 'plagu', 'narriat', 'llife', 'unpleasantli', 'shirt…', 'directio', 'driven', 'song', 'aroung', 'saysown', 'blizzard', \"mane'uv\", 'hillsi', 'moral', 'dray', 'trede', 'becos', 'worr', 'gorgeu', 'refesh', 'unclean', 'troubling…i', 'so-cal', 'wheater', 'hum', 'overtak', 'multitud', 'increasinlgli', 'hellish', 'make.', 'inteact', 'cone', 'nile', 'literari', 'techniqu', 'histori', 'person1.', 'deslat', 'beast', 'personalith', 'uncivil', 'hunt', 'uncultur', 'coloni', 'simultan', 't.v', 'tani', 'revel', 'braveri', 'hort', 'prolli', 'bounc', 'changing…', 'west', 'wold', 'absolut', 'sine', 'olden', 'peddal', 'colaps', 'hormon', 'glow', 'previous', 'vie', 'optic', 'bumpili', 'weird', 'withought', 'hister', 'thirst.th', 'brackish.', 'rich', 'contest', 'sub', 'earb', 'breakway', 'sayin', 'nonstop', 'sammertim', 'acid…', '…in', 'tombl', 'stoy', 'thoug', 'acctulli', 'thirdli', 'bull', 'trait', 'lonel', 'nass', 'ulcom', 'plass', 'interrupt', 'lengthi', 'water…', 'inturn', 'adis', 'guest', 'exhuber', 'tirerd', 'easyer', 'dtwepend', 'heartycl', 'moretir', 'scourch', 'eldrerli', 'fourth', 'decreased.h', 'town.on', 'topof', 'thatin', 'roadwa', 'denyar', 'couldstil', 'arizona', 'northern', 'danc', 'emic', 'continuesli', 'damper', 'oth', 'poach', 'cultist', 'worthwhil', 'famin', 'malnourish', 'reselt', 'cycolist', 'recogn', 'wrongli', 'modi', 'defect', 'stuch', 'sadden', 'happend', 'devold', 'evil', 'ramshockl', 'several', 'corrol', 'wesstern', \"'sinc\", '.distanc', 'spars', 'despir', 'determand', 'tean', 'bilav', 'realeov', 'earthley', 'oblig', 'eigther', 'miley', 'whollm', 'hagon', 'abig', 'california…', 'evidenc', 'goup', 'hills…', 'seveal']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def count_vectorizer(dataframe, column='tokens'): #Input is the dataframe we created above with three columns\n",
        "\n",
        "    from collections import Counter # import the Counter function\n",
        "\n",
        "    dataframe[column] = dataframe[column].apply(Counter).apply(dict) #we apply the counter function to the \"cleaned\" columns\n",
        "                                                                          # we used the apply function in week 2\n",
        "\n",
        "    bow = pd.DataFrame(dataframe[column].tolist()).T.fillna(0) # we restructure the output as a dataframe and name it \"bow\"\n",
        "                                                                  # also fill in the missing values (nan -> 0 )\n",
        "    unique_vocab = list(bow.index)                                # FYI, bow standards for (bag-of-words)\n",
        "\n",
        "    return bow, unique_vocab\n",
        "\n",
        "bow, unique_vocab = count_vectorizer(df, 'stem')\n",
        "print(bow)\n",
        "print(unique_vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae99de97",
      "metadata": {
        "id": "ae99de97"
      },
      "source": [
        "\n",
        "\n",
        "> *Let's explore!*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "aa575f5c",
      "metadata": {
        "id": "aa575f5c",
        "outputId": "23ac5cc6-a2a7-42e3-f4ef-6a5cfa58eaf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        54.0\n",
            "1       226.0\n",
            "2       110.0\n",
            "3        93.0\n",
            "4       149.0\n",
            "        ...  \n",
            "1721     80.0\n",
            "1722     61.0\n",
            "1723    129.0\n",
            "1724    176.0\n",
            "1725    134.0\n",
            "Length: 1726, dtype: float64\n",
            "space\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "the            16432.0\n",
              "featur          1375.0\n",
              "of              4913.0\n",
              "set             2439.0\n",
              "affect          2061.0\n",
              "                ...   \n",
              "california…        1.0\n",
              "evidenc            1.0\n",
              "goup               1.0\n",
              "hills…             1.0\n",
              "seveal             1.0\n",
              "Length: 5095, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# the most frequent word?\n",
        "print(bow.sum())\n",
        "\n",
        "print(\"space\")\n",
        "\n",
        "bow.sum(axis = 1)\n",
        "# axis = 1 switches the axis the function is applied to,\n",
        "## so instead of summing how many words each student wrote\n",
        "## we get the sum of how many times each word was used"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3eaac6f",
      "metadata": {
        "id": "b3eaac6f"
      },
      "source": [
        "### 1.2. Term-Frequency Inverse Documenr-Frequency Matrix\n",
        "\n",
        "       > Term-Freqeuncy: tf(t,d) = log(count(t,d)+1)\n",
        "       > Inverse-Document Frequency: idf(t) = log(N/df(t))\n",
        "       > TF-IDF: tf(t,d) X idf(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "379fa1aa",
      "metadata": {
        "id": "379fa1aa"
      },
      "outputs": [],
      "source": [
        "def tfidf_vectorizer(bow):\n",
        "\n",
        "    # ======================== STEP 1 ============================ #\n",
        "\n",
        "    # Step 1: we will compute the term-frequency (TF)\n",
        "\n",
        "    for response in bow.columns:\n",
        "        bow[response] = bow[response].apply(lambda x: np.log10(x+1))\n",
        "\n",
        "    step1_output = bow\n",
        "\n",
        "    # ======================== STEP 2 ============================ #\n",
        "\n",
        "    # Step 2: we will compute the inverse-document frequency (IDF)\n",
        "\n",
        "    df = pd.DataFrame( ( ( bow > 0 ) * 1 ).sum(axis=1), columns=['df'] )  #this is the document-freqeuncy\n",
        "    N = bow.shape[1] # this is the collection frequency (total number of doucments)\n",
        "    df['IDF'] = df['df'].apply(lambda x: np.log10(N/x))\n",
        "    step2_output= df\n",
        "\n",
        "    # ======================== STEP 3 ============================ #\n",
        "\n",
        "    # Step 3: term-freqeuncy (TF) * inverse-document frequency (IDF) = TF-IDF\n",
        "\n",
        "    tfidf = bow.multiply(df.IDF, axis='index')\n",
        "\n",
        "    return tfidf, step1_output, step2_output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f852d505",
      "metadata": {
        "id": "f852d505"
      },
      "source": [
        "> *Let's explore!*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "2c45f407",
      "metadata": {
        "id": "2c45f407"
      },
      "outputs": [],
      "source": [
        "## The _, mutes the second and third outputs, allowing you to just save the first tfidf output\n",
        "## as the second and third outputs are only useful to check\n",
        "tfidf, _, _ = tfidf_vectorizer(bow)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf"
      ],
      "metadata": {
        "id": "TYQIK7Uvkp9O",
        "outputId": "a562beba-c8ba-4db9-e792-f60244cb5f7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "id": "TYQIK7Uvkp9O",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 0         1         2         3         4         5     \\\n",
              "the          0.000882  0.001097  0.000757  0.000882  0.001070  0.000847   \n",
              "featur       0.044321  0.029899  0.000000  0.029899  0.000000  0.053546   \n",
              "of           0.010687  0.010687  0.000000  0.012351  0.013510  0.009503   \n",
              "set          0.016461  0.011104  0.011104  0.000000  0.022365  0.016461   \n",
              "affect       0.027669  0.027669  0.018666  0.018666  0.000000  0.018666   \n",
              "...               ...       ...       ...       ...       ...       ...   \n",
              "california…  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "evidenc      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "goup         0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "hills…       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "seveal       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "\n",
              "                6         7         8         9     ...      1716      1717  \\\n",
              "the          0.00062  0.000806  0.000882  0.000963  ...  0.000939  0.000912   \n",
              "featur       0.00000  0.000000  0.000000  0.000000  ...  0.029899  0.000000   \n",
              "of           0.00000  0.011606  0.007866  0.007866  ...  0.012975  0.007866   \n",
              "set          0.00000  0.000000  0.016461  0.000000  ...  0.011104  0.000000   \n",
              "affect       0.00000  0.033429  0.018666  0.027669  ...  0.027669  0.000000   \n",
              "...              ...       ...       ...       ...  ...       ...       ...   \n",
              "california…  0.00000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
              "evidenc      0.00000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
              "goup         0.00000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
              "hills…       0.00000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
              "seveal       0.00000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
              "\n",
              "                 1718      1719      1720      1721      1722      1723  \\\n",
              "the          0.000000  0.000985  0.001023  0.000847  0.000697  0.000939   \n",
              "featur       0.000000  0.053546  0.029899  0.000000  0.029899  0.000000   \n",
              "of           0.005306  0.010687  0.009503  0.007866  0.007866  0.010687   \n",
              "set          0.000000  0.019887  0.011104  0.011104  0.011104  0.016461   \n",
              "affect       0.000000  0.027669  0.018666  0.018666  0.018666  0.033429   \n",
              "...               ...       ...       ...       ...       ...       ...   \n",
              "california…  0.000000  0.000000  0.369953  0.000000  0.000000  0.000000   \n",
              "evidenc      0.000000  0.000000  0.369953  0.000000  0.000000  0.000000   \n",
              "goup         0.000000  0.000000  0.000000  0.000000  0.369953  0.000000   \n",
              "hills…       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "seveal       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "\n",
              "                 1724      1725  \n",
              "the          0.001005  0.000963  \n",
              "featur       0.029899  0.029899  \n",
              "of           0.007866  0.010687  \n",
              "set          0.016461  0.016461  \n",
              "affect       0.027669  0.018666  \n",
              "...               ...       ...  \n",
              "california…  0.000000  0.000000  \n",
              "evidenc      0.000000  0.000000  \n",
              "goup         0.000000  0.000000  \n",
              "hills…       0.369953  0.000000  \n",
              "seveal       0.369953  0.000000  \n",
              "\n",
              "[5095 rows x 1726 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9d0e9fb7-6add-4a6f-b121-f4c5d20cd670\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>1716</th>\n",
              "      <th>1717</th>\n",
              "      <th>1718</th>\n",
              "      <th>1719</th>\n",
              "      <th>1720</th>\n",
              "      <th>1721</th>\n",
              "      <th>1722</th>\n",
              "      <th>1723</th>\n",
              "      <th>1724</th>\n",
              "      <th>1725</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>the</th>\n",
              "      <td>0.000882</td>\n",
              "      <td>0.001097</td>\n",
              "      <td>0.000757</td>\n",
              "      <td>0.000882</td>\n",
              "      <td>0.001070</td>\n",
              "      <td>0.000847</td>\n",
              "      <td>0.00062</td>\n",
              "      <td>0.000806</td>\n",
              "      <td>0.000882</td>\n",
              "      <td>0.000963</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000939</td>\n",
              "      <td>0.000912</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000985</td>\n",
              "      <td>0.001023</td>\n",
              "      <td>0.000847</td>\n",
              "      <td>0.000697</td>\n",
              "      <td>0.000939</td>\n",
              "      <td>0.001005</td>\n",
              "      <td>0.000963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>featur</th>\n",
              "      <td>0.044321</td>\n",
              "      <td>0.029899</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.029899</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.053546</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.029899</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.053546</td>\n",
              "      <td>0.029899</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.029899</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.029899</td>\n",
              "      <td>0.029899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>of</th>\n",
              "      <td>0.010687</td>\n",
              "      <td>0.010687</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.012351</td>\n",
              "      <td>0.013510</td>\n",
              "      <td>0.009503</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.011606</td>\n",
              "      <td>0.007866</td>\n",
              "      <td>0.007866</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012975</td>\n",
              "      <td>0.007866</td>\n",
              "      <td>0.005306</td>\n",
              "      <td>0.010687</td>\n",
              "      <td>0.009503</td>\n",
              "      <td>0.007866</td>\n",
              "      <td>0.007866</td>\n",
              "      <td>0.010687</td>\n",
              "      <td>0.007866</td>\n",
              "      <td>0.010687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>set</th>\n",
              "      <td>0.016461</td>\n",
              "      <td>0.011104</td>\n",
              "      <td>0.011104</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.022365</td>\n",
              "      <td>0.016461</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.016461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011104</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.019887</td>\n",
              "      <td>0.011104</td>\n",
              "      <td>0.011104</td>\n",
              "      <td>0.011104</td>\n",
              "      <td>0.016461</td>\n",
              "      <td>0.016461</td>\n",
              "      <td>0.016461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>affect</th>\n",
              "      <td>0.027669</td>\n",
              "      <td>0.027669</td>\n",
              "      <td>0.018666</td>\n",
              "      <td>0.018666</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.018666</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.033429</td>\n",
              "      <td>0.018666</td>\n",
              "      <td>0.027669</td>\n",
              "      <td>...</td>\n",
              "      <td>0.027669</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.027669</td>\n",
              "      <td>0.018666</td>\n",
              "      <td>0.018666</td>\n",
              "      <td>0.018666</td>\n",
              "      <td>0.033429</td>\n",
              "      <td>0.027669</td>\n",
              "      <td>0.018666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>california…</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.369953</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>evidenc</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.369953</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>goup</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.369953</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hills…</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.369953</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>seveal</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.369953</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5095 rows × 1726 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9d0e9fb7-6add-4a6f-b121-f4c5d20cd670')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9d0e9fb7-6add-4a6f-b121-f4c5d20cd670 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9d0e9fb7-6add-4a6f-b121-f4c5d20cd670');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shortcut (You can use this for your project)"
      ],
      "metadata": {
        "id": "4JvnaUuXmULO"
      },
      "id": "4JvnaUuXmULO"
    },
    {
      "cell_type": "code",
      "source": [
        "## We don't need to do this by hand, scikit-learn\n",
        "\n",
        "## !pip install scikit-learn\n",
        "\n",
        "###### Count vectorizer #######\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "\n",
        "vectorizer = CountVectorizer() # define the vectorizer function\n",
        "\n",
        "input = df.response.values\n",
        "print(input[:3])\n",
        "X = vectorizer.fit_transform(input) # fit the data - the input is a list of string (list of essays)\n",
        "\n",
        "print(X.toarray())\n",
        "print(vectorizer.get_feature_names_out()) # this will print out the terms\n",
        "print(X.toarray().shape)\n",
        "\n",
        "############ TF-IDF #################\n",
        "## Same purpose as above, but using TF-IDF weighted vector rather than simple count vectorizing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(input)\n",
        "\n",
        "print(X.toarray())\n",
        "print(vectorizer.get_feature_names_out()) # this will print out the terms\n",
        "print(X.toarray().shape)\n",
        "\n",
        "## Note: TD-IDF does not overcome sparsity, in fact it can sometimes make it worse\n",
        "## it just places emphasis on the words that differentiate documents and minimizes\n",
        "## the influence of Ziph words"
      ],
      "metadata": {
        "id": "7AJNR5YJmTtO",
        "outputId": "d6b9d4fc-df7f-4fb4-9673-27344e49a7df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "7AJNR5YJmTtO",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The features of the setting affect the cyclist in many ways. The features of the setting that affect the cyclist is the lact of information on were to go and the lack of water. This was a problem because he needed water for his trip and directions on were to go.  '\n",
            " 'The features of the setting affected the cyclist in a negative way. He was in the desert, which is dry and hot. “I was traveling through the high deserts of California in June.\"(Kurmaskie @NUM1). Also, the  temperature was hot and made every thing around him hot, “brackish water faling somewhere in the neighborhood of two hundred degrees\" (Kurmaskie @NUM1). Since he was in the desert, there was no escape from the sun. The text  states, “The sun was beginning to beat down”(Kurmaskie @NUM3). The cyclist was all alone in the road with none around him either so into he passed out, no one could help him. “ and the growing realization that I could drop from heatstroke on a gorgous day in June”(Kurmaskie @NUM1). \"About forty miles into the pedal, I arrived at the first ‘town’ but on that morning it fit the traditional definition of a ghost town.\" (Kurmaskie @NUM3). Thesettings that the cyclist was riding in wasn\\'t good ones and so they negativly affected him; causing him to be dehydrated and tiredin the end.'\n",
            " 'Everyone travels to unfamiliar places. Sometimes we get lost and ask locals for directions which @MONTH1 not be a good idea. The setting affected the cyclist. He had a perfectly good map but asked older men who look like they haven’t been out in ages. The old men apparently haven’t been out because they gave the cyclist the wrong directions.  My advice would be to not listen to anyone no matter their age if you have a perfectly good map with you. Also try to know where you are going at all times. “Yes, sir!']\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "['12' '55' 'aase' ... 'yuuth' 'zone' 'zoo']\n",
            "(1726, 6348)\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "['12' '55' 'aase' ... 'yuuth' 'zone' 'zoo']\n",
            "(1726, 6348)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba5d3885",
      "metadata": {
        "id": "ba5d3885"
      },
      "source": [
        "## 2. Text Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15b1fb94",
      "metadata": {
        "id": "15b1fb94"
      },
      "source": [
        "<img src=\"https://i.pinimg.com/736x/2e/aa/7d/2eaa7d5021ca7c3c98bc93b98b9646fe.jpg\" align=\"left\" width=\"70\" height=\"70\" align=\"left\">\n",
        "\n",
        "> ## Task 1: Text Similarity\n",
        "\n",
        "> The function below (`compare_similarity`) takes in the dataframe (`data`) as an input then computes the average cosine similarity between all pairs of documents in the dataframe. You can specify the type of the matrix you want to use to compute the similarity as well (`types: {'bow', 'tfidf'}`). For example, we can do `compare_similarty(data, types='bow')`\n",
        ">> Q1. Let's randomly sample 10 documents from each score category (0 to 3) and compare the average similarity for the following two vectorization approaches. Then, explain the findings briefly.\n",
        ">> - `BOW [score 0:       score 1:        score 2:          score 3:       ]`\n",
        ">> - `TF-IDF [score 0:       score 1:        score 2:          score 3:       ]`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e9b0b36",
      "metadata": {
        "id": "8e9b0b36"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(v1, v2):\n",
        "    import math\n",
        "    \"compute cosine similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)\"\n",
        "\n",
        "    sumxx, sumxy, sumyy = 0, 0, 0\n",
        "\n",
        "    for i in range(len(v1)):\n",
        "        x = v1[i]; y = v2[i]\n",
        "        sumxx += x*x\n",
        "        sumyy += y*y\n",
        "        sumxy += x*y\n",
        "\n",
        "    return sumxy/math.sqrt(sumxx*sumyy)\n",
        "\n",
        "def compare_similarity(matrix, types='bow'):\n",
        "\n",
        "    if types=='bow':\n",
        "        bow, _ = count_vectorizer(matrix)\n",
        "        final = bow\n",
        "    elif types=='tfidf':\n",
        "        bow, _ = count_vectorizer(matrix)\n",
        "        tfidf, _, _ = tfidf_vectorizer(bow)\n",
        "        final = tfidf\n",
        "    else:\n",
        "        print('we will use the default==bow')\n",
        "        bow, _ = count_vectorizer(matrix)\n",
        "        final = bow\n",
        "\n",
        "    from itertools import combinations\n",
        "    sim = 0\n",
        "    combi = list(combinations(list(final.columns), 2))\n",
        "    for (i,j) in combi:\n",
        "        temp = cosine_similarity(final[i].T, final[j].T)\n",
        "        sim +=temp\n",
        "    ave_sim = sim/len(combi)\n",
        "    return ave_sim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d018778",
      "metadata": {
        "id": "3d018778",
        "outputId": "30d18bc9-607f-4696-a169-284e28645cce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.31662083985533535"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "##################### YOUR CODE HERE ##############################\n",
        "\n",
        "# Average cosine similarity between the randomly sampled 10 documents in the score category 0 (using BOW)\n",
        "# step 1\n",
        "score0 = df[df.score==0] #select the data from the score category 0\n",
        "# step 2\n",
        "random0 = score0.sample(10) # we are using the `df.sample(n)` function to randomly sample a total of N rows.\n",
        "# step 3\n",
        "compare_similarity(random0) # get the average similarity score\n",
        "\n",
        "# Average cosine similarity between the randomly sampled 10 documents in the score category 1 (using BOW)\n",
        "\n",
        "\n",
        "# Average cosine similarity between the randomly sampled 10 documents in the score category 2 (using BOW)\n",
        "\n",
        "\n",
        "# Average cosine similarity between the randomly sampled 10 documents in the score category 3 (using BOW)\n",
        "\n",
        "\n",
        "\n",
        "###################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4450b276",
      "metadata": {
        "id": "4450b276"
      },
      "outputs": [],
      "source": [
        "##################### YOUR CODE HERE ##############################\n",
        "\n",
        "# Average cosine similarity between the randomly sampled 10 documents in the score category 0 (using TF-IDF)\n",
        "\n",
        "\n",
        "# Average cosine similarity between the randomly sampled 10 documents in the score category 1 (using TF-IDF)\n",
        "\n",
        "\n",
        "# Average cosine similarity between the randomly sampled 10 documents in the score category 2 (using TF-IDF)\n",
        "\n",
        "\n",
        "# Average cosine similarity between the randomly sampled 10 documents in the score category 3 (using TF-IDF)\n",
        "\n",
        "\n",
        "\n",
        "###################################################################"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N1JvAF-TqYLT"
      },
      "id": "N1JvAF-TqYLT",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}